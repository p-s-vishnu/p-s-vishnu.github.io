<!doctype html>
<html lang="en" dir="ltr" class="blog-wrapper blog-post-page plugin-blog plugin-id-default" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.9.2">
<title data-rh="true">Why Your LLM Is Slow (And the 5 Papers That Fix It) | Vishnu P.S.</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://p-s-vishnu.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://p-s-vishnu.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://p-s-vishnu.github.io/why-your-llm-is-slow"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docusaurus_tag" content="default"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docsearch:docusaurus_tag" content="default"><meta data-rh="true" property="og:title" content="Why Your LLM Is Slow (And the 5 Papers That Fix It) | Vishnu P.S."><meta data-rh="true" name="description" content="Recently came across a post explaining these papers and thought it worth sharing with a quick breakdown."><meta data-rh="true" property="og:description" content="Recently came across a post explaining these papers and thought it worth sharing with a quick breakdown."><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2026-02-22T00:00:00.000Z"><meta data-rh="true" property="article:tag" content="LLM,MLOps"><link data-rh="true" rel="icon" href="/img/old/icon_minion.png"><link data-rh="true" rel="canonical" href="https://p-s-vishnu.github.io/why-your-llm-is-slow"><link data-rh="true" rel="alternate" href="https://p-s-vishnu.github.io/why-your-llm-is-slow" hreflang="en"><link data-rh="true" rel="alternate" href="https://p-s-vishnu.github.io/why-your-llm-is-slow" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://0T9WWE13Y1-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","@id":"https://p-s-vishnu.github.io/why-your-llm-is-slow","mainEntityOfPage":"https://p-s-vishnu.github.io/why-your-llm-is-slow","url":"https://p-s-vishnu.github.io/why-your-llm-is-slow","headline":"Why Your LLM Is Slow (And the 5 Papers That Fix It)","name":"Why Your LLM Is Slow (And the 5 Papers That Fix It)","description":"Recently came across a post explaining these papers and thought it worth sharing with a quick breakdown.","datePublished":"2026-02-22T00:00:00.000Z","author":[],"keywords":[],"isPartOf":{"@type":"Blog","@id":"https://p-s-vishnu.github.io/","name":"Blog"}}</script><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Vishnu P.S. RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Vishnu P.S. Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J8413B0NWF"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-J8413B0NWF",{anonymize_ip:!0})</script>



<link rel="search" type="application/opensearchdescription+xml" title="Vishnu P.S." href="/opensearch.xml">

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous">
<link rel="preconnect" href="https://cdnjs.cloudflare.com"><link rel="stylesheet" href="/assets/css/styles.5f87fc15.css">
<script src="/assets/js/runtime~main.7aa4a550.js" defer="defer"></script>
<script src="/assets/js/main.7b561cc8.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",t||"light"),document.documentElement.setAttribute("data-theme-choice",t||"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_x0vj" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/old/icon_minion.png" alt="Vishnu&#x27;s Portfolio Logo" class="themedComponent_HFBj themedComponent--light_mT0Q"><img src="/img/old/icon_minion.png" alt="Vishnu&#x27;s Portfolio Logo" class="themedComponent_HFBj themedComponent--dark_IUiX"></div><b class="navbar__title text--truncate">Vishnu P.S.</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a class="navbar__item navbar__link" href="/notes">Notes</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Blog</a><a class="navbar__item navbar__link" href="/about">About</a><div class="toggle_g_IE colorModeToggle_SHqq"><button class="clean-btn toggleButton_ctbn toggleButtonDisabled_t4fw" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_sCRX lightToggleIcon__dj0"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_sCRX darkToggleIcon_d0Ec"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_sCRX systemToggleIcon_n1Ky"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_wksd"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Meta+k)" aria-keyshortcuts="Meta+k"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 24 24" aria-hidden="true"><circle cx="11" cy="11" r="8" stroke="currentColor" fill="none" stroke-width="1.4"></circle><path d="m21 21-4.3-4.3" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_X2RI"><div class="container margin-vert--lg"><div class="row"><aside class="col col--3"><nav class="sidebar_iubh thin-scrollbar" aria-label="Blog recent posts navigation"><div class="sidebarItemTitle_JODL margin-bottom--md">Recent posts</div><div role="group"><h3 class="yearGroupHeading_nAMp">2026</h3><ul class="sidebarItemList_hsCD clean-list"><li class="sidebarItem_PJRp"><a aria-current="page" class="sidebarItemLink_bRXC sidebarItemLinkActive_e1Uy" href="/why-your-llm-is-slow">Why Your LLM Is Slow (And the 5 Papers That Fix It)</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_nAMp">2025</h3><ul class="sidebarItemList_hsCD clean-list"><li class="sidebarItem_PJRp"><a class="sidebarItemLink_bRXC" href="/building-a-data-analysis-agent">Building a Production Data Analysis Agent</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_nAMp">2024</h3><ul class="sidebarItemList_hsCD clean-list"><li class="sidebarItem_PJRp"><a class="sidebarItemLink_bRXC" href="/2024/01/06/Monzo-AWS-reInvent">Learnings from Monzo: AWS reInvent A Deep Dive into Building a Digital Bank</a></li></ul></div><div role="group"><h3 class="yearGroupHeading_nAMp">2022</h3><ul class="sidebarItemList_hsCD clean-list"><li class="sidebarItem_PJRp"><a class="sidebarItemLink_bRXC" href="/2022/07/25/how-to-frame-an-ml-problem-part2">Part 2: How to frame an ML problem?</a></li><li class="sidebarItem_PJRp"><a class="sidebarItemLink_bRXC" href="/2022/07/14/how-to-frame-an-ml-problem-part1">Part 1: How to frame an ML problem?</a></li></ul></div></nav></aside><main class="col col--7"><article class=""><header><h1 class="title__Z_6">Why Your LLM Is Slow (And the 5 Papers That Fix It)</h1><div class="container_zXXG margin-vert--md"><time datetime="2026-02-22T00:00:00.000Z">February 22, 2026</time> · <!-- -->5 min read</div></header><div id="__blog-post-container" class="markdown"><p>Recently came across a post explaining these papers and thought it worth sharing with a quick breakdown.</p>
<h2 class="anchor anchorTargetStickyNavbar_xRYd" id="tldr">TLDR<a href="#tldr" class="hash-link" aria-label="Direct link to TLDR" title="Direct link to TLDR" translate="no">​</a></h2>
<table><thead><tr><th>Concept</th><th>Layer</th><th>Key Win</th><th>Remember This</th></tr></thead><tbody><tr><td>FlashAttention</td><td>Compute</td><td>2-6x attention speedup</td><td>Tiling + IO-awareness</td></tr><tr><td>PagedAttention</td><td>Memory</td><td>Less than 4% waste (was 60-80%)</td><td>Virtual memory for KV cache</td></tr><tr><td>Speculative Decoding</td><td>Generation</td><td>2-3.6x faster decoding</td><td>Draft-then-verify</td></tr><tr><td>Heterogeneous Serving</td><td>Infrastructure</td><td>Up to 77% cost savings</td><td>Right GPU for right job</td></tr><tr><td>DistServe</td><td>Architecture</td><td>7.4x more requests</td><td>Split prefill from decoding</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_xRYd" id="1-flashattention---compute-optimisation">1. FlashAttention - Compute Optimisation<a href="#1-flashattention---compute-optimisation" class="hash-link" aria-label="Direct link to 1. FlashAttention - Compute Optimisation" title="Direct link to 1. FlashAttention - Compute Optimisation" translate="no">​</a></h2>
<p><strong>The problem:</strong> Attention is slow not because of maths, but because of memory traffic. GPUs have fast on-chip memory (SRAM, ~19TB/s) and slow main memory (HBM, ~2TB/s). Standard attention keeps shuffling data between them.</p>
<p><strong>How it works:</strong> Instead of computing the full N×N attention matrix at once, FlashAttention tiles it into small blocks that fit in fast SRAM. It uses an &quot;online softmax&quot; trick to get exact results incrementally - no approximation.</p>
<p><strong>Analogy:</strong> Instead of carrying all your groceries inside in one impossible armful, you make smart small trips - but you planned the route so well it&#x27;s actually faster.</p>
<table><thead><tr><th>Version</th><th>Speedup</th><th>GPU Utilisation</th></tr></thead><tbody><tr><td>v1</td><td>2-3x vs standard</td><td>25-40%</td></tr><tr><td>v2</td><td>2x on top of v1</td><td>50-73%</td></tr></tbody></table>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_xRYd" id="2-pagedattention---memory-management">2. PagedAttention - Memory Management<a href="#2-pagedattention---memory-management" class="hash-link" aria-label="Direct link to 2. PagedAttention - Memory Management" title="Direct link to 2. PagedAttention - Memory Management" translate="no">​</a></h2>
<p><strong>The problem:</strong> Each request stores a KV cache (the model&#x27;s &quot;memory&quot; of past tokens). Traditional systems pre-allocate memory for the worst case, wasting 60-80% of GPU memory. This limits how many requests you can batch together.</p>
<p><strong>How it works:</strong> Borrows the virtual memory paging concept from operating systems. KV cache is split into fixed-size blocks that can be scattered anywhere in GPU memory. A block table maps logical to physical locations. Memory is allocated on-demand as tokens are generated.</p>
<p><strong>Analogy:</strong> Instead of reserving an entire bookshelf per person (wasteful), you let people&#x27;s books sit on any available shelf and give them a card catalogue to find them.</p>
<table><thead><tr><th>Metric</th><th>Result</th></tr></thead><tbody><tr><td>Memory waste</td><td>60-80% to less than 4%</td></tr><tr><td>Throughput vs HuggingFace</td><td>24x</td></tr><tr><td>Throughput vs prior SOTA</td><td>2-4x</td></tr></tbody></table>
<p><strong>Why it matters:</strong> Memory efficiency means larger batch sizes, more requests per GPU, and lower cost. This is why vLLM became the industry standard for LLM serving.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_xRYd" id="3-speculative-decoding---faster-token-generation">3. Speculative Decoding - Faster Token Generation<a href="#3-speculative-decoding---faster-token-generation" class="hash-link" aria-label="Direct link to 3. Speculative Decoding - Faster Token Generation" title="Direct link to 3. Speculative Decoding - Faster Token Generation" translate="no">​</a></h2>
<p><strong>The problem:</strong> LLMs generate tokens one at a time, each requiring a full forward pass through billions of parameters. The GPU is massively underutilised - like hiring 1,000 workers to carry one brick at a time.</p>
<p><strong>Two approaches:</strong></p>
<p><strong>Speculative Sampling (DeepMind):</strong> A small fast &quot;draft&quot; model guesses the next k tokens. The big model verifies all k in one forward pass. If the guesses match, you got k tokens for the price of roughly 1. Mathematically guaranteed to produce identical output.</p>
<p><strong>Medusa (Cai et al., Princeton/UIUC):</strong> Instead of a separate model, bolt extra &quot;prediction heads&quot; onto the main model. Each head predicts future tokens in parallel. Simpler deployment (one model), but requires fine-tuning.</p>
<table><thead><tr><th>Approach</th><th>Speedup</th><th>Trade-off</th></tr></thead><tbody><tr><td>Speculative Sampling</td><td>2-2.5x</td><td>Need two models</td></tr><tr><td>Medusa-2</td><td>2.3-3.6x</td><td>Need to fine-tune heads</td></tr></tbody></table>
<p><strong>Analogy:</strong> Instead of asking the CEO to write a memo word by word, have an intern draft 5 sentences, then the CEO reviews them all at once - keeping what&#x27;s good, rewriting what&#x27;s not.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_xRYd" id="4-heterogeneous-gpu-serving---cost-optimisation">4. Heterogeneous GPU Serving - Cost Optimisation<a href="#4-heterogeneous-gpu-serving---cost-optimisation" class="hash-link" aria-label="Direct link to 4. Heterogeneous GPU Serving - Cost Optimisation" title="Direct link to 4. Heterogeneous GPU Serving - Cost Optimisation" translate="no">​</a></h2>
<p><strong>The problem:</strong> Companies buy expensive A100s for everything, but not all requests need top-tier hardware. Short chat messages don&#x27;t need the same GPU as processing 100-page documents.</p>
<p><strong>Two approaches:</strong></p>
<p><strong>Metis (training-focused):</strong> Automatically figures out how to split training across mixed GPU types with smart load balancing. Result: 1-8.4x speedup.</p>
<p><strong>Melange (inference-focused):</strong> Formulates GPU selection as a bin-packing problem - which mix of cheap and expensive GPUs minimises cost while meeting latency targets?</p>
<table><thead><tr><th>Workload</th><th>Cost Savings</th></tr></thead><tbody><tr><td>Short chat</td><td>up to 77%</td></tr><tr><td>Long documents</td><td>33%</td></tr><tr><td>Mixed</td><td>51%</td></tr></tbody></table>
<p><strong>Analogy:</strong> Instead of sending limousines for every taxi ride, dispatch the right vehicle for each trip - sedans for solo riders, vans for groups.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_xRYd" id="5-distserve---disaggregated-inference">5. DistServe - Disaggregated Inference<a href="#5-distserve---disaggregated-inference" class="hash-link" aria-label="Direct link to 5. DistServe - Disaggregated Inference" title="Direct link to 5. DistServe - Disaggregated Inference" translate="no">​</a></h2>
<p><strong>The problem:</strong> LLM inference has two phases with opposite needs:</p>
<ul>
<li class=""><strong>Prefill</strong> (process the prompt): wants massive parallelism, high throughput</li>
<li class=""><strong>Decoding</strong> (generate tokens): wants low latency, small batches</li>
</ul>
<p>Running both on the same GPU is like asking one chef to do both bulk meal prep and delicate plating simultaneously - neither goes well.</p>
<p><strong>How it works:</strong> Physically separate prefill and decoding onto different GPU clusters, each tuned for its workload. After prefill generates the KV cache, it is shipped to the decoding cluster.</p>
<table><thead><tr><th>Metric</th><th>Result</th></tr></thead><tbody><tr><td>Goodput vs vLLM (requests meeting SLO/s)</td><td>7.4x more</td></tr><tr><td>SLO compliance</td><td>Over 90% of requests meet latency targets</td></tr></tbody></table>
<p><strong>Analogy:</strong> A restaurant with a separate prep kitchen (high-volume chopping) and a plating station (precision finishing), connected by a runner.</p>
<hr>
<h2 class="anchor anchorTargetStickyNavbar_xRYd" id="references">References<a href="#references" class="hash-link" aria-label="Direct link to References" title="Direct link to References" translate="no">​</a></h2>
<ol>
<li class="">Dao et al. (2022) : <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noopener noreferrer" class="">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a></li>
<li class="">Dao (2023) : <a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noopener noreferrer" class="">FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning</a></li>
<li class="">Kwon et al. (2023) : <a href="https://arxiv.org/abs/2309.06852" target="_blank" rel="noopener noreferrer" class="">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></li>
<li class="">Chen et al. (2023) : <a href="https://arxiv.org/abs/2211.17192" target="_blank" rel="noopener noreferrer" class="">Accelerating Large Language Model Decoding with Speculative Sampling</a></li>
<li class="">Cai et al. (2024) : <a href="https://arxiv.org/abs/2401.02659" target="_blank" rel="noopener noreferrer" class="">Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads</a></li>
<li class="">Um et al. (2022) : <a href="https://arxiv.org/abs/2208.14226" target="_blank" rel="noopener noreferrer" class="">Metis: Fast Automatic Distributed Training on Heterogeneous GPUs</a></li>
<li class="">Griggs et al. (2024) : <a href="https://arxiv.org/abs/2404.14527" target="_blank" rel="noopener noreferrer" class="">Mélange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity</a></li>
<li class="">Zhong et al. (2024) : <a href="https://arxiv.org/abs/2401.09670" target="_blank" rel="noopener noreferrer" class="">DistServe: Disaggregating Prefill and Decoding for Goodput-Optimized LLM Serving</a></li>
</ol></div><footer class="docusaurus-mt-lg"><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col"><b>Tags:</b><ul class="tags_kQYD padding--none margin-left--sm"><li class="tag_T9CG"><a rel="tag" title="LLM, LLMs, etc." class="tag_U6it tagRegular_Kgam" href="/tags/llm">LLM</a></li><li class="tag_T9CG"><a rel="tag" title="MLOps, end to end lifecycle" class="tag_U6it tagRegular_Kgam" href="/tags/mlops">MLOps</a></li></ul></div></div><div class="row margin-top--sm theme-blog-footer-edit-meta-row"><div class="col noPrint_ioEn"><a href="https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2026-02-22-why-your-llm-is-slow.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_o0CY" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_lnRI"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Blog post page navigation"><a class="pagination-nav__link pagination-nav__link--next" href="/building-a-data-analysis-agent"><div class="pagination-nav__sublabel">Older post</div><div class="pagination-nav__label">Building a Production Data Analysis Agent</div></a></nav></main><div class="col col--2"><div class="tableOfContents_R1gp thin-scrollbar"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#tldr" class="table-of-contents__link toc-highlight">TLDR</a></li><li><a href="#1-flashattention---compute-optimisation" class="table-of-contents__link toc-highlight">1. FlashAttention - Compute Optimisation</a></li><li><a href="#2-pagedattention---memory-management" class="table-of-contents__link toc-highlight">2. PagedAttention - Memory Management</a></li><li><a href="#3-speculative-decoding---faster-token-generation" class="table-of-contents__link toc-highlight">3. Speculative Decoding - Faster Token Generation</a></li><li><a href="#4-heterogeneous-gpu-serving---cost-optimisation" class="table-of-contents__link toc-highlight">4. Heterogeneous GPU Serving - Cost Optimisation</a></li><li><a href="#5-distserve---disaggregated-inference" class="table-of-contents__link toc-highlight">5. DistServe - Disaggregated Inference</a></li><li><a href="#references" class="table-of-contents__link toc-highlight">References</a></li></ul></div></div></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/psvishnu/" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a></li><li class="footer__item"><a href="https://medium.com/@psvishnu" target="_blank" rel="noopener noreferrer" aria-label="Medium" title="Medium"><i class="fa-brands fa-medium"></i></a></li><li class="footer__item"><a href="mailto:hellovishnups@gmail.com" aria-label="Contact" title="Contact"><i class="fa-solid fa-envelope"></i></a></li><li class="footer__item"><a href="https://github.com/p-s-vishnu" target="_blank" rel="noopener noreferrer" aria-label="GitHub" title="GitHub"><i class="fa-brands fa-github"></i></a></li><li class="footer__item"><a href="https://www.kaggle.com/psvishnu" target="_blank" rel="noopener noreferrer" aria-label="Kaggle" title="Kaggle"><i class="fa-brands fa-kaggle"></i></a></li><li class="footer__item"><a href="/rss.xml" target="_blank" rel="noopener noreferrer" aria-label="RSS Feed" title="RSS Feed"><i class="fa-solid fa-rss"></i></a></li></ul></div></div></div></footer></div>
</body>
</html>