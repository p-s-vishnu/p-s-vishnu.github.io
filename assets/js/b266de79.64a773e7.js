"use strict";(self.webpackChunkportfolio=self.webpackChunkportfolio||[]).push([[3518],{4369:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2024/01/06/Monzo-AWS-reInvent","metadata":{"permalink":"/2024/01/06/Monzo-AWS-reInvent","editUrl":"https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2024-01-06-Monzo-AWS-reInvent.md","source":"@site/blog/2024-01-06-Monzo-AWS-reInvent.md","title":"Learnings from Monzo: AWS reInvent A Deep Dive into Building a Digital Bank","description":"Let\u2019s take a sneak peek into the world of Monzo, the digital banking rocking around 8 million accounts, mostly in the UK. When they hit 4 million customers there were just eight tech folks on the infrastructure and reliability team running the show on AWS.","date":"2024-01-06T00:00:00.000Z","tags":[],"readingTime":7.77,"hasTruncateMarker":true,"authors":[],"frontMatter":{},"unlisted":false,"nextItem":{"title":"Online prediction using GCP\u2019s Vertex AI","permalink":"/online-prediction-using-gcp-vertex-ai"}},"content":"Let\u2019s take a sneak peek into the world of Monzo, the digital banking rocking around 8 million accounts, mostly in the UK. When they hit 4 million customers there were just eight tech folks on the infrastructure and reliability team running the show on AWS.\\n\\n\x3c!-- truncate --\x3e\\n\\nI watched the\xa0[video](https://www.youtube.com/watch?v=NTgB2z0E9ZU)\xa0and read about the\xa0[case study](https://aws.amazon.com/solutions/case-studies/Monzo/)\xa0of Monzo and below are the key highlights extracted from them. I have added the pre-requisite and further reading topics in quotations.\\n\\nThe article is divided into two sections, For those who want the quick low-down, the TLDR has got you covered with the gist of the content but if you are up for a more in-depth explanation then the next one does that.\\n\\n**_Note:_**\xa0_Most of the screenshots are from the links above._\\n\\n## TL;DR\\n\\nIn a nutshell, the setup is humming 1500 micro-services and ~ 9000 pods in production as of 2019.\\n\\n**Infra overview \u2014**\xa0Below are the tools and system architecture for the payment system\\n\\n1. **_Compute_**\xa0Data Centres (DCs) process incoming requests which get redirected to Micro-services running on\xa0[Kubernetes](https://kubernetes.io/).\\n2. **_Data store:_**\xa0[Apache Cassandra](https://cassandra.apache.org/_/index.html)\xa0& AWS S3 for persistent data storage\\n3. **_Monitoring and alerting:_**\xa0[Prometheus](https://prometheus.io/docs/introduction/overview/)\xa0for monitoring & unlimited retention with\xa0[Thanos](https://thanos.io/)\\n4. **_Distributed locking and coordination:_**\xa0[etcd](https://etcd.io/)\xa0cluster\\n5. **_Ordered Queuing:_**\xa0[Apache Kafka](https://kafka.apache.org/)\\n6. **_Unordered queuing & event publishing:_**\xa0[NSQ](https://nsq.io/)\\n\\n![Monzo Infra](/img/blog/monzo-infra.png)\\n\\nWhat happens when a Payment request is initiated?\\n\\n1. The request for card usage is sent from the payment provider to Monzo\u2019s DCs.\\n2. The request is then transferred to AWS and some requests would need a locking mechanism using etcd. Simultaneously, some will be sent to the Cassandra cluster for the data through the Kubernetes compute cluster.\\n3. If the transaction is approved then send back the response.\\n\\n![Monzo Payment](/img/blog/monzo-payments.png)\\n\\n\\n## Deep Dive\\n\\nSix aspects of Monzo\u2019s payment system will be discussed\\n\\n1. Data Centres\\n2. Compute\\n3. Data storage\\n4. Messaging\\n5. Locking\\n6. Monitoring\\n\\nFinally, the overall request-response process.\\n\\n![Monzo Deep Dive](/img/blog/monzo-deep-dive.png)\\n\\n## 1. Data Centres\\n\\n> **What is AWS Direct Connect?**  \\n> It serves as a bridge between an internal network (on-prem) to an AWS Direct Connect location (AWS services e.g. Amazon S3 or Amazon VPC).\\n\\nThe necessity for a data centre arises from the limitation posed by payment providers like\xa0[Mastercard](https://www.mastercard.co.uk/en-gb/html)\xa0and\xa0[Faster Payments](https://www.starlingbank.com/resources/banking/guide-to-faster-payments/)\xa0which exclusively provide optical fibre and lack integration capability with cloud providers.\\n\\n**Request flow:**\\n\\n1. _Ingress from the service provider:_\xa0The journey commences with a message inbound from the service provider.\\n2. _Monzo\u2019s Data Centre:_\xa0The message traverses through Monzo\u2019s data centre, where it undergoes encryption and is channelled into a Virtual Private Network (VPN) through AWS Direct Connect.\\n3. Finally reaches the Kubernetes(K8s) cluster where it gets authenticated and passes through multiple micro-services.\\n\\n![Monzo Data Centre](/img/blog/monzo-data-centre.png)\\n\\n**Response:** It follows a similar reverse path and is returned to the terminal.\\n\\nIf you want to know how Monzo secures its application for IP compliance you can read the article \u2014 [How we secure Monzo\u2019s banking platform](https://monzo.com/blog/2022/03/31/how-we-secure-monzos-banking-platform).\\n\\n## 2. Compute\\n\\nAs mentioned earlier, there is a fleet of 1500 micro-services and ~ 9000 pods in production as of 2019 mostly in single region\xa0`eu-west-1`.\\n\\nThe question arises: How are they able to run so many nodes in the cluster?\\n\\n- It converges to one reason \u2014 consistency. Across teams, the company upholds a uniformity of language, design patterns, and infrastructure. This enables the orchestration of a multitude of nodes seamlessly.\\n- Additionally, all the code is written in Go which helps reduce the docker image size and expedites the container creation process.\\n\\nFor context, the below is not a neural network but rather micro-services running and communicating on a single day.\\n\\n![Monzo Compute](/img/blog/monzo-compute.png)\\n\\nThe shipper is the tool employed at Monzo to build, validate and roll out deployments to the cluster.\\n\\n![Monzo Shipper](/img/blog/monzo-shipper.png)\\n\\nA typical deployment command looks like below and there are hundreds of similar deployments done in a day.\\n\\n![Monzo Deployment](/img/blog/monzo-deployment.png)\\n\\n**Service discovery**\xa0using Envoy\\n\\n> [**Envoy**](https://www.envoyproxy.io/docs/envoy/latest/intro/what_is_envoy)**:**\xa0serves as a versatile software, functioning as a service proxy/mesh to govern and oversee both inbound and outbound traffic for all services within the service mesh.\\n\\n- Since there are numerous deployments, it becomes important to devise a mechanism for the services to find and communicate with one another, this is solved using envoy and custom logic called envoy config provider.\\n- The config provider monitors state changes in the K8s API server and subsequently, it orchestrates updates across all proxy processes and lets them know what to find where.\\n\\n\\n![Monzo Envoy](/img/blog/monzo-envoy.png)\\n\\n\\n## **3. Data Storage**\\n\\n> **How does Cassandra work?**  \\n> Cassandra operates as a Masterless distributed database, it has nodes that are joined to form a ring \u21d2 Data spans across these interconnected rings.  \\n> Operations like reading happen using a load-balancing mechanism like round-robin and the client remains oblivious to the data\u2019s actual whereabouts. Read more\xa0[here](https://cassandra.apache.org/_/cassandra-basics.html)\\n\\nAccount data is stored in the NoSQL Cassandra database and all the log archives are stored in the S3 bucket. It is running outside of the K8s cluster on the Ec2 instance.\\n\\nTo fine-tune the database\u2019s response time and consistency, replication and Quorum mechanisms come into play. For instance, if a need for a faster response arises, adjusting the replication factor becomes the strategy, albeit with the understanding that this may come at the cost of reduced consistency.\\n\\nDelving into the team\u2019s routine practices, certain exercises involve restarting the database cluster one node at a time. This deliberate action serves as a litmus test, assessing the resilience and robustness of the database in real-world scenarios.\\n\\n## **4. Messaging**\\n\\n> **What is event-driven architecture?  \\n> **A paradigm, where different components of a system communicate with each other through events. An event can be any occurrence or change in the system that requires attention. These events can be generated by users, applications, or external systems.\\n\\n\\n![Monzo Messaging](/img/blog/monzo-messaging.png)\\n\\n\\nA lot of computing works asynchronously, like an Event-driven architecture so messaging systems were introduced. In this system, Kafka is used for ordered queuing while NSQ is for unordered & event publishing scenarios. These threads provide interesting content about Kafka vs NSQ ([link1](https://news.ycombinator.com/item?id=14455919),\xa0[link2](https://gcore.com/learning/nats-rabbitmq-nsq-kafka-comparison/))\\n\\nPush notification is one scenario where asynchronous messaging is used. Whenever a customer makes a successful purchase or a vendor requests approval for payment a notification is instantaneously sent to the customer\u2019s app.\\n\\n## **5. Locking**\\n\\n> **What are AWS I3 instances?**  \\n> Amazon EC2 I3 instances are Storage Optimised instances for high transaction, low latency workloads. I3 instances offer a good price per I/O performance for workloads such as NoSQL databases, in-memory databases, data warehousing, Elastic search, and analytics workloads.\\n> \\n> **What is etcd?  \\n> **Distributed, highly available, key-value store. High throughput and low latency locking. It works akin to Cassandra however unlike the former it has a master selection process.\\n\\nGiven the distributed nature, there are instances where mutual exclusivity and locking become imperative. Here,\xa0**etcd**\xa0providing distributed locking capabilities. Without proper locking, concurrent modifications can lead to inconsistent or corrupted data. Locks ensure that only one process can modify the data at a time, maintaining its integrity.\\n\\nAdditionally, running the setup on AWS I3 infrastructure guarantees better performance.\\n\\n## **6. Monitoring**\\n\\n> **Prometheus**\xa0is a Time series data store and query engine\\n> \\n> **Thanos**\xa0is a highly available Prometheus setup providing infinite retention capabilities.\\n> \\n> **Sidecar pattern:**\xa0The Sidecar pattern is a design pattern used in micro-services architecture. It involves running a separate process or container alongside the main application to enhance its functionality. Think of it like a sidecar attached to a motorcycle; the sidecar provides additional features to the bike.\xa0[Read more](https://learn.microsoft.com/en-us/azure/architecture/patterns/sidecar)\\n\\n\\n![Monzo Monitoring](/img/blog/monzo-monitoring.png)\\n\\nPrometheus is \u201cshared out\u201d to various functional domains with two replicas each to handle failures, the collected time series data is \u201cephemeral \u201c (only 24 hours of storage). This begs two requirements for the monitoring system.\\n\\nUsers necessitate an effortless means to query logs, preferably centralised for comprehensive search capabilities.\\nThe logs should be saved for longer than 24 hours.\\nSolution \u2014 Thanos as a sidecar to the Prometheus servers\\n\\nEmulates multiple Prometheus servers\u2019 query search as a single one with Thanos. It uses Thanos query which fans out to all the sidecar applications retrieving the location of the requested data i.e. parallely searching for the requested data.\\nTo meet the second requirement, Thanos takes periodic strides, saving data to the S3 bucket, ensuring a repository that surpasses the 24-hour timeframe\\nBeyond this, an array of metrics are monitored by the system like request metrics, low-level system metrics, business logic, social media trends, etc. Hence, various Alert managers are created using Prometheus to detect anomalies and observe trends."},{"id":"online-prediction-using-gcp-vertex-ai","metadata":{"permalink":"/online-prediction-using-gcp-vertex-ai","editUrl":"https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2022-05-22-online-prediction-using-gcp-vertex-ai.md","source":"@site/blog/2022-05-22-online-prediction-using-gcp-vertex-ai.md","title":"Online prediction using GCP\u2019s Vertex AI","description":"Serve and process real-time data with a Tensorflow model using Pub-Sub, Cloud Dataflow, BigQuery and Vertex AI.","date":"2022-05-22T00:00:00.000Z","tags":[{"inline":false,"label":"Use Case","permalink":"/tags/usecase","description":"Company Use Cases, whitepaper, product, etc."},{"inline":false,"label":"Machine Learning","permalink":"/tags/machine-learning","description":"Classical Machine Learning"}],"readingTime":8.64,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"online-prediction-using-gcp-vertex-ai","title":"Online prediction using GCP\u2019s Vertex AI","tags":["usecase","machine-learning"]},"unlisted":false,"prevItem":{"title":"Learnings from Monzo: AWS reInvent A Deep Dive into Building a Digital Bank","permalink":"/2024/01/06/Monzo-AWS-reInvent"},"nextItem":{"title":"Common Product Metrics","permalink":"/product-metrics"}},"content":"Serve and process real-time data with a Tensorflow model using Pub-Sub, Cloud Dataflow, BigQuery and Vertex AI.\\n\\n\x3c!-- truncate --\x3e\\n\\n# Online prediction using GCP\u2019s Vertex AI\\n\\nIn this project, we are predicting the travel fare when a user books a cab. Unlike traditional pricing calculation, here the price is calculated dynamically based on multiple parameters, including the feature **number of trips in the last 5 minutes**, which acts as a proxy for real-time traffic.\\n\\n## Table of Content\\n\\n1. Architecture\\n2. Data Ingestion\\n3. Model Deployment\\n4. Prediction\\n\\n## 1. Architecture\\n\\n![GCP\u2019s ASL repository](/img/blog/vertexai-architecture.png)\\n\\nThe whole system can be divided into two parts\\n\\n1. Ingesting continuous feed of taxi trip data\\n2. Predicting on-demand fare requests\\n\\n## 2. Data Ingestion\\nFor the data ingestion part, we will need Pub/Sub, Dataflow and Big query. Let\'s configure the required services one by one.\\n\\n### Pub-Sub\\n> Pub/Sub is used for streaming analytics and data integration pipelines to ingest and distribute data. It is equally effective as a messaging-oriented middleware for service integration or as a queue to parallelize tasks. [Read more about the core concepts like - topics, subscription, publisher, etc.](https://cloud.google.com/pubsub/docs/overview#core_concepts)\\n\\nHere Pub/Sub will be used as a messaging bus that receives and stores recently completed taxi trips.\\n\\n```python title=\\"pubsub.py\\"\\nimport logging\\nfrom google import api_core\\nfrom google.cloud import pubsub_v1 as pubsub\\n\\n\\ndef get_pubsub_client(gcp_project_id, topic=\\"taxi_rides\\"):\\n    \\"\\"\\" Get topic if already exists or else create a new one\\n    \\"\\"\\"\\n    publisher = pubsub.PublisherClient()\\n    topic_name = publisher.topic_path(gcp_project_id, topic)\\n    try:\\n        publisher.get_topic(topic_name)\\n        logging.info(\\"Reusing pub/sub topic %s\\", topic)\\n    except api_core.exceptions.NotFound:\\n        publisher.create_topic(topic_name)\\n        logging.info(\\"Creating pub/sub topic %s\\", topic)\\n    return publisher\\n```\\n\\nSince we cannot get live taxi trip data we will create a python script that randomly generates trip data and pushes it to the Pub/Sub.\\n\\n```shell\\nPROJECT_ID=$(gcloud config get-value project)\\nREGION=$(gcloud config get-value ai/region)\\nBUCKET=$PROJECT_ID # change as necessary\\n```\\n\\nRun the following code as a separate script, it is configured to send about 2,000 trip messages every five minutes with some randomness in the frequency to mimic traffic fluctuations. These numbers come from looking at the historical average of taxi ride frequency in BigQuery.\\n\\nIn production this script would be replaced with actual taxis with IoT devices sending trip data to Cloud Pub/Sub.\\n\\n```python\\npublisher = get_pubsub_client(PROJECT_ID, topic=\\"taxi_rides\\")\\nwhile True:\\n  num_trips = random.randint(10, 60)\\n  for i in range(num_trips):\\n    publisher.publish(topic_name, b\\"taxi_ride\\")\\n  logging.info(\\"Publishing: %s\\", time.ctime())\\n  time.sleep(5)\\n```\\n\\n### Big Query\\n\\n> Big Query is a Data warehouse managed by google to store, process, analyse, and visualize large amounts of data. [Link to read more](https://cloud.google.com/bigquery/docs/introduction).\\n\\n```python title=\\"bigquery.py\\"\\nimport logging\\nfrom google import api_core\\nfrom google.cloud import bigquery\\n\\n\\ndef create_dataset(dataset_id=\\"taxifare\\"):\\n    bq = bigquery.Client()\\n    dataset = bigquery.Dataset(bq.dataset(dataset_id))\\n    try:\\n        bq.create_dataset(dataset)  # will fail if dataset already exists\\n        logging.info(\\"Dataset created.\\")\\n    except api_core.exceptions.Conflict:\\n        logging.info(\\"Dataset already exists.\\")\\n\\ndef create_table(dataset_id=\\"taxifare\\", table_name=\\"traffic_realtime\\"):\\n    bq = bigquery.Client()\\n    dataset = bigquery.Dataset(bq.dataset(dataset_id))\\n    table_ref = dataset.table(table_name)\\n    SCHEMA = [\\n        bigquery.SchemaField(\\"trips_last_5min\\", \\"INTEGER\\", mode=\\"REQUIRED\\"),\\n        bigquery.SchemaField(\\"time\\", \\"TIMESTAMP\\", mode=\\"REQUIRED\\"),\\n    ]\\n    table = bigquery.Table(table_ref, schema=SCHEMA)\\n    try:\\n        bq.create_table(table)\\n        logging.info(\\"Table created.\\")\\n    except api_core.exceptions.Conflict:\\n        logging.info(\\"Table already exists.\\")\\n```\\n\\n### Dataflow\\n\\n> [Dataflow](https://www.youtube.com/watch?v=XdsuDOQ9nkU&ab_channel=GoogleCloudTech) is a fast, [serverless](https://www.youtube.com/watch?v=vxJobGtqKVM&ab_channel=IBMTechnology) service for executing batch and streaming data processing pipelines. You create your pipelines with Apache Beam and then run them using the Dataflow service.\\n[Link to basics of Apache beam](https://beam.apache.org/documentation/basics/) & [Link to read more about Dataflow](https://cloud.google.com/dataflow).\\n\\nNow that our taxi data is pushed to Pub/Sub, and our BigQuery table is set up, let\u2019s consume the Pub/Sub data using a streaming DataFlow pipeline.\\n\\nDataflow will be responsible for the following transformations:\\n\\n1. Pull the completed trips from the Pub/Sub\\n2. Window the messages (every 5 mins)\\n3. Count the number of messages in the window\\n4. Format the count for BigQuery\\n5. Write results to BigQuery table\\n\\n```python title=\\"dataflow.py\\"\\nimport argparse\\nfrom datetime import datetime\\n\\nimport apache_beam as beam\\nfrom apache_beam.options.pipeline_options import (\\n    GoogleCloudOptions,\\n    PipelineOptions,\\n    SetupOptions,\\n    StandardOptions,\\n)\\nfrom apache_beam.transforms import window  # pylint: disable=unused-import\\n\\n\\nclass CountFn(beam.CombineFn):\\n    \\"\\"\\"Counter function to accumulate statistics\\"\\"\\"\\n    def create_accumulator(self):\\n        return 0\\n\\n    def add_input(self, count, element):\\n        del element\\n        return count + 1\\n\\n    def merge_accumulators(self, accumulators):\\n        return sum(accumulators)\\n\\n    def extract_output(self, count):\\n        return count\\n\\n\\ndef run(argv=None):\\n    \\"\\"\\"Build and run the pipeline.\\"\\"\\"\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\n        \\"--project\\", help=(\\"Google Cloud Project ID\\"), required=True\\n    )\\n    parser.add_argument(\\"--region\\", help=(\\"Google Cloud region\\"), required=True)\\n    parser.add_argument(\\n        \\"--input_topic\\",\\n        help=(\\"Google Cloud PubSub topic name \\"),\\n        required=True,\\n    )\\n    known_args, pipeline_args = parser.parse_known_args(argv)\\n    pipeline_options = PipelineOptions(pipeline_args)\\n    pipeline_options.view_as(SetupOptions).save_main_session = True\\n    pipeline_options.view_as(StandardOptions).streaming = True\\n    pipeline_options.view_as(GoogleCloudOptions).region = known_args.region\\n    pipeline_options.view_as(GoogleCloudOptions).project = known_args.project\\n    p = beam.Pipeline(options=pipeline_options)\\n    \\n    topic = f\\"projects/{known_args.project}/topics/{known_args.input_topic}\\"\\n    # this table needs to exist\\n    table_spec = f\\"{known_args.project}:taxifare.traffic_realtime\\"\\n\\n    def to_bq_format(count):\\n        \\"\\"\\"BigQuery writer requires rows to be stored as python dictionary\\"\\"\\"\\n        return {\\n            \\"trips_last_5min\\": count,\\n            \\"time\\": datetime.now().strftime(\\"%Y-%m-%d %H:%M:%S\\"),\\n        }\\n    pipeline = (  # noqa F841 pylint: disable=unused-variable\\n        p\\n        | \\"read_from_pubsub\\"\\n        >> beam.io.ReadFromPubSub(topic=topic).with_output_types(bytes)\\n        | \\"window\\"\\n        >> beam.WindowInto(window.SlidingWindows(size=300, period=15))\\n        | \\"count\\" >> beam.CombineGlobally(CountFn()).without_defaults()\\n        | \\"format_for_bq\\" >> beam.Map(to_bq_format)\\n        | \\"write_to_bq\\"\\n        >> beam.io.WriteToBigQuery(\\n            table_spec,\\n            # WRITE_TRUNCATE not supported for streaming\\n            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\\n            create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER,\\n        )\\n    )\\n    # result.wait_until_finish() #only do this if running with DirectRunner\\n    result = p.run()  # noqa F841 pylint: disable=unused-variable\\n```\\n\\nLaunch the dataflow pipeline using the command below.\\n\\n```shell\\npython3 dataflow.py --input_topic taxi_rides --runner=DataflowRunner --project=$PROJECT_ID --region=$REGION --temp_location=gs://$BUCKET/dataflow_streaming\\n```\\n\\n## 3. Model Deployment\\n### Vertex AI\\n\\n> [Vertex AI](https://www.youtube.com/watch?v=gT4qqHMiEpA&ab_channel=GoogleCloudTech)\xa0is a Jupyter-based fully managed, scalable, enterprise-ready compute infrastructure with security controls and user management capabilities. It serves as a one-stop environment to complete all of the ML work, from experimentation to deployment, to managing and monitoring models.\xa0[Link to read more](https://cloud.google.com/vertex-ai)\\n\\nFor keeping the article short, I have refrained from explaining the training code. The code can be found in the below GitHub repo. Do let me know in the comments if an explanation of the training code would help.\\n\\n![Vertex AI Pipeline](/img/blog/vertexai-pipeline.png)\\n\\n### Kubeflow pipeline\\n\\n> [Kubeflow](https://www.youtube.com/watch?v=cTZArDgbIWw&ab_channel=GoogleCloudTech)\xa0is known as the ML toolkit for\xa0[Kubernetes](https://kubernetes.io/). The project is dedicated to making deployments of Machine Learning (ML) workflows on Kubernetes simple, portable, and scalable. The goal is to provide a straightforward way to deploy best-of-breed open-source systems for ML in diverse infrastructures.  \\n> [Link to read more](https://www.kubeflow.org/docs/started/architecture/)\\n\\nWe will make our prediction service available now. For that, we will be wrapping the following processes as a kubeflow pipeline.\\n\\n1. **Model training:**\xa0Based on the latest labelled data start the model training as a serverless job.\\n2. **Upload model:**\xa0The output of the trained model will be transferred to Google cloud storage.\\n3. **Create endpoint:**\xa0While the model is being trained create an endpoint for the same asynchronously.\\n4. **Deployment:**\xa0Finally deploy the model to the created endpoint.\\n\\n\\n```python title=\\"train_upload_deploy_pipeline.py\\"\\nfrom kfp.v2.dsl import component, pipeline\\nfrom kfp.v2.google import experimental\\nfrom google_cloud_pipeline_components.aiplatform import ModelUploadOp, EndpointCreateOp, ModelDeployOp\\n\\n\\n@component\\ndef training_op(input1: str):\\n    print(f\\"VertexAI pipeline: {input1}\\")\\n\\n@pipeline(name=\\"taxifare--train-upload-endpoint-deploy\\")\\ndef pipeline(\\n    project: str = PROJECT,\\n    model_display_name: str = MODEL_DISPLAY_NAME,\\n):\\n    # 1. Model Training\\n    train_task = training_op(\\"taxifare training pipeline\\")\\n    experimental.run_as_aiplatform_custom_job(\\n        train_task,\\n        display_name=f\\"pipelines-train-{TIMESTAMP}\\",\\n        worker_pool_specs=[\\n            {\\n                \\"pythonPackageSpec\\": {\\n                    \\"executor_image_uri\\": PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\\n                    \\"package_uris\\": [PYTHON_PACKAGE_URIS],\\n                    \\"python_module\\": PYTHON_MODULE,\\n                    \\"args\\": [\\n                        f\\"--eval_data_path={EVAL_DATA_PATH}\\",\\n                        f\\"--output_dir={OUTDIR}\\",\\n                        f\\"--train_data_path={TRAIN_DATA_PATH}\\",\\n                        f\\"--batch_size={BATCH_SIZE}\\",\\n                        f\\"--num_examples_to_train_on={NUM_EXAMPLES_TO_TRAIN_ON}\\",  # noqa: E501\\n                        f\\"--num_evals={NUM_EVALS}\\",\\n                        f\\"--nbuckets={NBUCKETS}\\",\\n                        f\\"--lr={LR}\\",\\n                        f\\"--nnsize={NNSIZE}\\",\\n                    ],\\n                },\\n                \\"replica_count\\": f\\"{REPLICA_COUNT}\\",\\n                \\"machineSpec\\": {\\n                    \\"machineType\\": f\\"{MACHINE_TYPE}\\",\\n                },\\n            }\\n        ],\\n    )\\n\\n    # 2. Model Upload\\n    model_upload_op = ModelUploadOp(\\n        project=f\\"{PROJECT}\\",\\n        display_name=f\\"pipelines-ModelUpload-{TIMESTAMP}\\",\\n        artifact_uri=f\\"{OUTDIR}/savedmodel\\",\\n        serving_container_image_uri=f\\"{SERVING_CONTAINER_IMAGE_URI}\\",\\n        serving_container_environment_variables={\\"NOT_USED\\": \\"NO_VALUE\\"},\\n    )\\n    model_upload_op.after(train_task)\\n\\n    # 3. Create Endpoint\\n    endpoint_create_op = EndpointCreateOp(\\n        project=f\\"{PROJECT}\\",\\n        display_name=f\\"pipelines-EndpointCreate-{TIMESTAMP}\\",\\n    )\\n\\n    # 4. Deployment\\n    model_deploy_op = ModelDeployOp(\\n        project=f\\"{PROJECT}\\",\\n        endpoint=endpoint_create_op.outputs[\\"endpoint\\"],\\n        model=model_upload_op.outputs[\\"model\\"],\\n        deployed_model_display_name=f\\"{MODEL_DISPLAY_NAME}\\",\\n        machine_type=f\\"{MACHINE_TYPE}\\",\\n    )\\n    \\n# Compile the pipeline\\nfrom kfp.v2 import compiler\\n\\nif not os.path.isdir(\\"vertex_pipelines\\"):\\n    os.mkdir(\\"vertex_pipelines\\")\\n\\ncompiler.Compiler().compile(\\n    pipeline_func=pipeline,\\n    package_path=\\"./vertex_pipelines/train_upload_endpoint_deploy.json\\",\\n```\\n\\n\\nIf the compilation is successful then you can run the following code and your pipeline will start running.\\n\\n```python title=\\"run_kfpipeline.py\\"\\n# Run the pipeline\\nfrom google_cloud_pipeline_components import aiplatform\\n\\npipeline_job = aiplatform.pipeline_jobs.PipelineJob(\\n    display_name=\\"taxifare_pipeline\\",\\n    template_path=\\"./vertex_pipelines/train_upload_endpoint_deploy.json\\",\\n    pipeline_root=f\\"{PIPELINE_ROOT}\\",\\n    project=PROJECT,\\n    location=REGION,\\n)\\npipeline_job.run()\\n```\\n\\nOnce the pipeline starts, head to Vertex AI > Pipeline and you should be able to see the pipeline similar to the below screenshot.\\n\\n![Vertex AI Pipeline](/img/blog/vertexai-pipeline.png)\\n\\n## 4. Prediction\\n\\nFrom the previous step, save the endpoint(ENDPOINT) where our model is deployed. We need only two more components to make this system complete.\\n\\n1. A function that would fetch the last 5 min traffic and add it as a feature to the request.\\n2. Another function is to pass on the modified request, create the prediction service client and present the result.\\n\\n```python title=\\"fare_prediction.py\\"\\nimport logging\\nfrom typing import Dict, List, Union\\n\\nfrom google.cloud import aiplatform, bigquery\\nfrom google.protobuf import json_format\\nfrom google.protobuf.struct_pb2 import Value\\n\\ndef add_traffic_last_5min(instance, dataset=\\"taxifare\\", table=\\"traffic_realtime\\"):\\n    \\"\\"\\" Adds the dynamic feature `traffic_last_5min` to the instance\\n    \\"\\"\\"\\n    bq = bigquery.Client()\\n    query_string = f\\"\\"\\"\\n    SELECT\\n      *\\n    FROM\\n      `{dataset}.{table}`\\n    ORDER BY\\n      time DESC\\n    LIMIT 1\\n    \\"\\"\\"\\n    trips = bq.query(query_string).to_dataframe()[\\"trips_last_5min\\"][0]\\n    instance[\\"traffic_last_5min\\"] = int(trips)\\n    return instance\\n\\ndef predict(\\n    project: str,\\n    endpoint_id: str,\\n    instances: Union[Dict, List[Dict]],\\n    location: str = \\"us-central1\\",\\n    api_endpoint: str = \\"us-central1-aiplatform.googleapis.com\\",\\n):\\n    \\"\\"\\"\\n    `instances` can be either single instance of type dict or a list\\n    of instances.\\n    Reference: https://github.com/googleapis/python-aiplatform/blob/master/samples/snippets/predict_custom_trained_model_sample.py\\n    \\"\\"\\"\\n    client_options = {\\"api_endpoint\\": api_endpoint}     # f\\"{REGION}-aiplatform.googleapis.com\\"\\n    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\\n    \\n    instances = instances if type(instances) == list else [instances]\\n    instances = [add_traffic_last_5min(instance_dict) for instance_dict in instances]\\n    instances = [\\n        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\\n    ]\\n    parameters_dict = {}\\n    parameters = json_format.ParseDict(parameters_dict, Value())\\n    endpoint = client.endpoint_path(\\n        project=project, location=location, endpoint=endpoint_id\\n    )\\n    response = client.predict(\\n        endpoint=endpoint, instances=instances, parameters=parameters\\n    )\\n    logging.info(\\"response\\")\\n    logging.info(f\\" deployed_model_id: {response.deployed_model_id}\\")\\n    \\n    # The predictions are a google.protobuf.Value representation of the model\'s predictions.\\n    predictions = response.predictions\\n    for prediction in predictions:\\n        logging.info(\\" prediction:\\", dict(prediction))\\n```\\n\\nNow you are ready to hit the endpoint and receive the prediction.\\n\\n```python\\ninstance = { \\n    \\"dayofweek\\": 4, \\n    \\"hourofday\\": 13, \\n    \\"pickup_longitude\\":-73.99, \\n    \\"pickup_latitude\\": 40.758, \\n    \\"dropoff_latitude\\": 41.742, \\n    \\"dropoff_longitude\\": -73.07\\n}\\npredict(PROJECT_ID, ENDPOINT, instance)\\n\\n```\\n\\nYou can refer to my repository for the complete code: [\ud83d\udcbb Code](https://github.com/p-s-vishnu/taxibooking-vertexai)."},{"id":"product-metrics","metadata":{"permalink":"/product-metrics","editUrl":"https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2021-11-29-product-metrics.md","source":"@site/blog/2021-11-29-product-metrics.md","title":"Common Product Metrics","description":"Some of the common product metrics used in product management.","date":"2021-11-29T00:00:00.000Z","tags":[{"inline":false,"label":"Use Case","permalink":"/tags/usecase","description":"Company Use Cases, whitepaper, product, etc."}],"readingTime":1.64,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"product-metrics","title":"Common Product Metrics","tags":["usecase"]},"unlisted":false,"prevItem":{"title":"Online prediction using GCP\u2019s Vertex AI","permalink":"/online-prediction-using-gcp-vertex-ai"},"nextItem":{"title":"Booking.com\'s RS","permalink":"/booking-com-rs"}},"content":"Some of the common product metrics used in product management.\\n\\n\x3c!-- truncate --\x3e\\n# Main Types of metrics\\n### 1. **User engagement metrics**\\n\\n   - Number of sessions per user.\\n\\n   - Sessions duration for a cohorts/group, over time\\n\\n   - Number of key user actions per session. eg: number of likes, etc\\n\\n   - CTR (Click-through rate)\\n\\n   ```math\\n   (Number of click / Total Impressions) * 100\\n   ```\\n\\n### 2. **Business metrics**\\n\\n   - ROI: Return on Investment - `The value is in %`\\n\\n   ```math\\n   ((Gain from investment \u2013 Cost of Investment) / Cost of Investment ) * 100\\n   ```\\n\\n   - LTV/CLV: Customer Lifetime Value.\\n\\n    Net profit you will generate until the customer churns or stops paying for service. Alternate metrics is Customer Lifetime Revenue, where one excludes the customer servicing cost (salary of employees, server cost, etc).\\n\\n   - CAC: Customer Acquisition Cost.\\n\\n    It is the ratio of (Sales and marketing expense)/(number of new customers).\\n\\n   - ARPA: Average revenue per account\\n\\n   - MRR: Monthly recurring revenue\\n\\n   - Revenue churn rate\\n\\n   Percentage of revenue loss per month due to churn or stoppage of service.\\n\\n   - Retention rate [not a good metric]\\n\\n   Far less actionable than other metric like Churn metric\\n\\n### 3. **Customer service metrics**\\n\\n   - Number of incoming support ticket\\n\\n   - FCR: First Call Resolution\\n\\n   - Net Promoter Score\\n\\n        ```math\\n        Promoters % - Detractors %\\n        ```\\n        > Range: -100% to 100%\\n\\n### 4. **Churn analysis**\\n\\n\\n:::tip Tips\\nFor counts, take Medians over Means as they are less sensitive to outliers.\\n\\nChurn analysis, do a t-test of the mean churned and retained users.\\n:::\\n\\n### References\\n\\n1. [Critical Metrics Every Product Manager Must Track](https://productcoalition.com/critical-metrics-every-product-manager-must-track-c5f1e46e3423)\\n\\n2. [How to Measure Growth: Cohort Analysis and Retention Rate](http://www.nirandfar.com/2013/12/are-you-focusing-too-much-on-growth-how-to-measure-habits.html)\\n\\n3. [Calculating Customer Retention (Fixed and Rolling)](http://www.newnorth.com/how-to-calculate-metrics-for-customer-retention/)\\n\\n4. [How to Calculate Customer Retention and Dollar Retention](http://www.evergage.com/blog/how-calculate-customer-retention/)\\n\\n5. [Measuring Customer Retention: Retention and Average Lifetime Customer Value](http://www.vindicia.com/measuring-customer-retention-key-metrics-matter/)\\n\\n6. [Source for WhatsApp Monthly Active Users]"},{"id":"booking-com-rs","metadata":{"permalink":"/booking-com-rs","editUrl":"https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2021-10-29-booking.com.md","source":"@site/blog/2021-10-29-booking.com.md","title":"Booking.com\'s RS","description":"Summary of Booking.com\'s RS, their Machine Learning Productionization System.","date":"2021-10-29T00:00:00.000Z","tags":[{"inline":false,"label":"MLOps","permalink":"/tags/mlops","description":"MLOps, end to end lifecycle"},{"inline":false,"label":"Machine Learning","permalink":"/tags/machine-learning","description":"Classical Machine Learning"}],"readingTime":1.8,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"booking-com-rs","title":"Booking.com\'s RS","tags":["mlops","machine-learning"]},"unlisted":false,"prevItem":{"title":"Common Product Metrics","permalink":"/product-metrics"}},"content":"Summary of Booking.com\'s RS, their Machine Learning Productionization System.\\n\\n\x3c!-- truncate --\x3e\\n\\n![Booking.com RS](/img/blog/booking-rs-tradeoff.png)\\n\\n\ud83d\uded0 One of the Core ideas of booking.com: \\"Diversity gives us strength\\".\\n\\n\ud83e\udd55 Requirements:\\n1. **Consistency:** online predictions and offline should match\\n2. **High availability:** system available 24/7\\n3. **Low latency:** real-time or near real-time predictions\\n4. **Scalability:** Should be able to handle multiple requests simultaneously.\\n5. **Observability:** Monitor input and output space.\\n6. **Reusability:** The same model can be used in multiple places eg: a family-friendly hotel predictor can be used on the home page and in many filters.\\n\\n\ud83d\udc68\u200d\ud83d\udc68\u200d\ud83d\udc66\u200d\ud83d\udc66 The fantastic four approach\\n1. **Lookup tables:**\\n\u25b4 Precompute all the possible input vectors and save them as key-value pairs.\\nFor a prediction, they have to just look up. It is Low latency, horizontally scaleable - Implemented using the Cassandra key-value store (or in memory if they are small enough).\\nDisadvantages:\\n\u25b4 Computation overhead and possibly resource wastage, even more, difficult with newer model versions.\\nUsage:\\n\u25b4 Discrete input space cases, User / accommodation / destination identifiers.\\n\\n2. **Generalized Linear Models (GLMs):**\\n\u25b4 The model representation is stored in the linear form of weight and bias and computed when needed. Can also be SVMs. So continuous input. - It doesn\u2019t matter which training algorithm is used as long as it can be represented by a weight vector, an input transformation and link function, it can be run in production.\\n\u25b4 Inner product < vectorize(X), W>\\n\u25b4 Argsort for ranking the results based on the score\\nDisadvantage:\\n\u25b4 An extra step, once trained convert to linear form.\\nUsage:\\n\u25b4 User context models, destination, recommendations, etc\\n\\n3. **Native libraries:**\\n\u25b4 Most straightforward approach: Train the model to serialize it and deserialize before serving.\\nDisadvantage\\n\u25b4 It has latency issues, this is usually optimized for training and not necessarily for serving.\\nUsage:\\n\u25b4 Tree-based model, GBT, NN\\n\\n4. **Scripted models:**\\n\u25b4 A script is invoked during the request which gives the flexibility to control output for cases like post-processing.\\n\u25b4 Flexibility for post-processing.\\nDisadvantage\\n\u25b4 Each line of code will determine the online request lifecycle latency.\\nUsage\\n\u25b4 For unsupported libraries and models with extra logic.\\n\\n**Reference:** [Booking.com\'s RS](https://booking.ai/https-booking-ai-machine-learning-production-3ee8fe943c70)"}]}}')}}]);