"use strict";(globalThis.webpackChunkportfolio=globalThis.webpackChunkportfolio||[]).push([[926],{8019(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>i,metadata:()=>s,toc:()=>d});var s=t(2748),r=t(6070),a=t(56);const i={slug:"building-a-data-analysis-agent",title:"Building a Production Data Analysis Agent",tags:["agents","llm","software-engineering"]},o="Building a Production Data Analysis Agent",l={authorsImageUrls:[]},d=[{value:"TL;DR",id:"tldr",level:2},{value:"What is a Data Analysis Agent?",id:"what-is-a-data-analysis-agent",level:2},{value:"Architecture",id:"architecture",level:2},{value:"Why These Frameworks?",id:"why-these-frameworks",level:2},{value:"LangGraph",id:"langgraph",level:3},{value:"FastAPI",id:"fastapi",level:3},{value:"Redis",id:"redis",level:3},{value:"LiteLLM",id:"litellm",level:3},{value:"MCP",id:"mcp",level:3},{value:"Implementation",id:"implementation",level:2},{value:"Docker Compose + LiteLLM",id:"docker-compose--litellm",level:3},{value:"Database Schema",id:"database-schema",level:3},{value:"Agent State + LangGraph Workflow",id:"agent-state--langgraph-workflow",level:3},{value:"SQL Tool",id:"sql-tool",level:3},{value:"Redis Memory",id:"redis-memory",level:3},{value:"FastAPI Routes",id:"fastapi-routes",level:3},{value:"MCP Server",id:"mcp-server",level:3},{value:"Running It",id:"running-it",level:2},{value:"References",id:"references",level:2}];function c(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",hr:"hr",img:"img",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.p,{children:"Build a data analysis agent that turns natural language into SQL, runs queries against a database, and returns insights -- using LangGraph, FastAPI, Redis, LiteLLM, and MCP. Everything runs locally via Docker Compose."}),"\n",(0,r.jsx)(n.h2,{id:"tldr",children:"TL;DR"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Natural language to SQL"})," agent backed by a sample e-commerce database"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LangGraph"})," for stateful workflow orchestration with conditional routing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"FastAPI"})," with SSE streaming"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Redis"})," for conversation memory and query caching"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiteLLM"})," proxy -- swap OpenAI, Anthropic, or Ollama without code changes"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"MCP"})," server for external tool access (Claude Desktop, Cursor)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Docker Compose"})," one-command setup"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.a,{href:"https://github.com/p-s-vishnu/data-agent",children:"Code: Companion code"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"what-is-a-data-analysis-agent",children:"What is a Data Analysis Agent?"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.img,{alt:"Autonomy levels",src:t(4744).A+"",width:"1794",height:"1150"})}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:["A ",(0,r.jsx)(n.strong,{children:"workflow"})," follows predefined paths -- the developer decides the control flow. An ",(0,r.jsx)(n.strong,{children:"agent"})," uses an LLM to decide what to do next."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["Our agent sits at ",(0,r.jsx)(n.strong,{children:"level 2"})," -- an ",(0,r.jsx)(n.strong,{children:"Orchestrator-Worker"})," pattern. A router classifies intent, then delegates to specialised workers (SQL generation, execution, analysis). We define the paths; the LLM picks which one."]}),"\n",(0,r.jsx)(n.p,{children:"Full autonomy (level 3) would be overkill here. We want deterministic tools orchestrated by an LLM that understands context."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,r.jsx)(n.mermaid,{value:"graph LR\n    User --\x3e|POST /api/chat| FastAPI\n    FastAPI --\x3e LangGraph\n    LangGraph --\x3e|classify| Router\n    Router --\x3e|data question| SQLGen[SQL Generator]\n    Router --\x3e|follow-up| Analyst\n    Router --\x3e|general| Responder\n    SQLGen --\x3e Executor[Query Executor]\n    Executor --\x3e Analyst\n    Analyst --\x3e Responder\n    Responder --\x3e|response| FastAPI\n\n    LangGraph <--\x3e|history| Redis\n    Executor <--\x3e|SELECT only| PostgreSQL\n    LangGraph <--\x3e|completions| LiteLLM[LiteLLM Proxy]\n    LiteLLM --\x3e LLM[LLM Provider]"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Request flow:"})}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"User sends a question to FastAPI."}),"\n",(0,r.jsx)(n.li,{children:"Conversation history is loaded from Redis."}),"\n",(0,r.jsxs)(n.li,{children:["The ",(0,r.jsx)(n.strong,{children:"router"})," classifies intent -- data question, follow-up, or general."]}),"\n",(0,r.jsxs)(n.li,{children:["For data questions: ",(0,r.jsx)(n.strong,{children:"SQL generator"})," -> ",(0,r.jsx)(n.strong,{children:"executor"})," -> ",(0,r.jsx)(n.strong,{children:"analyst"})," -> ",(0,r.jsx)(n.strong,{children:"responder"}),"."]}),"\n",(0,r.jsx)(n.li,{children:"Updated conversation is persisted back to Redis."}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"why-these-frameworks",children:"Why These Frameworks?"}),"\n",(0,r.jsx)(n.h3,{id:"langgraph",children:"LangGraph"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://langchain-ai.github.io/langgraph/",children:(0,r.jsx)(n.strong,{children:"LangGraph"})})," -- stateful, multi-step agent workflows as directed graphs."]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Conditional routing"})," -- the router branches to entirely different paths based on LLM classification. Plain chains cannot do this."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Shared state"})," -- ",(0,r.jsx)(n.code,{children:"AgentState"})," carries messages, SQL, results, and analysis across nodes without globals."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Loop-back"})," -- the analyst can request more data and route back to the SQL generator."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"fastapi",children:"FastAPI"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://fastapi.tiangolo.com/",children:(0,r.jsx)(n.strong,{children:"FastAPI"})})," -- async Python web framework built on Starlette and Pydantic."]}),"\n"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Native async for non-blocking DB calls and streaming."}),"\n",(0,r.jsx)(n.li,{children:"Auto-generated OpenAPI docs from Pydantic models."}),"\n",(0,r.jsxs)(n.li,{children:["SSE streaming via ",(0,r.jsx)(n.code,{children:"sse-starlette"})," for token-by-token responses."]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"redis",children:"Redis"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://redis.io/",children:(0,r.jsx)(n.strong,{children:"Redis"})})," -- in-memory data store."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Two use cases:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Conversation history"})," -- keyed by ",(0,r.jsx)(n.code,{children:"conversation:{id}"}),", 24h TTL."]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Query cache"})," -- keyed by SHA-256 of the SQL, 5min TTL. Avoids re-executing expensive queries during iterative analysis."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Sub-millisecond reads. Automatic expiry. No cleanup jobs."}),"\n",(0,r.jsx)(n.h3,{id:"litellm",children:"LiteLLM"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://docs.litellm.ai/",children:(0,r.jsx)(n.strong,{children:"LiteLLM"})})," -- proxy server providing a unified OpenAI-compatible API for 100+ LLM providers."]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:["The agent talks to ",(0,r.jsx)(n.code,{children:"http://litellm-proxy:4000/v1"}),". Whether that routes to GPT-4.1, Claude Sonnet, or local Ollama is purely config. Also gives you spend tracking, rate limiting, and fallbacks for free."]}),"\n",(0,r.jsx)(n.admonition,{type:"tip",children:(0,r.jsxs)(n.p,{children:["For Rust setups, ",(0,r.jsx)(n.a,{href:"https://docs.rs/litellm-rs/latest/litellm_rs/index.html",children:"litellm-rs"})," provides a similar interface with lower overhead."]})}),"\n",(0,r.jsx)(n.h3,{id:"mcp",children:"MCP"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://modelcontextprotocol.io/",children:(0,r.jsx)(n.strong,{children:"Model Context Protocol"})})," -- standardised protocol for connecting AI models with tools."]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Define tools once via MCP, any compatible client can discover and use them. No per-client integration work."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"implementation",children:"Implementation"}),"\n",(0,r.jsx)(n.h3,{id:"docker-compose--litellm",children:"Docker Compose + LiteLLM"}),"\n",(0,r.jsx)(n.p,{children:"Four services:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",metastring:'title="docker-compose.yml"',children:'services:\n  litellm-proxy:\n    image: ghcr.io/berriai/litellm:main-latest\n    ports:\n      - "4000:4000"\n    volumes:\n      - ./litellm/config.yaml:/app/config.yaml\n    command: ["--config", "/app/config.yaml"]\n    env_file: .env\n    healthcheck:\n      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - "6379:6379"\n\n  postgres:\n    image: postgres:16-alpine\n    environment:\n      POSTGRES_DB: ecommerce\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: postgres\n    volumes:\n      - ./db/init.sql:/docker-entrypoint-initdb.d/init.sql\n\n  agent-api:\n    build: .\n    ports:\n      - "8000:8000"\n    env_file: .env\n    environment:\n      LITELLM_BASE_URL: http://litellm-proxy:4000/v1\n      REDIS_URL: redis://redis:6379/0\n      DATABASE_URL: postgresql+asyncpg://postgres:postgres@postgres:5432/ecommerce\n    depends_on:\n      litellm-proxy:\n        condition: service_healthy\n      redis:\n        condition: service_started\n      postgres:\n        condition: service_started\n'})}),"\n",(0,r.jsx)(n.p,{children:"LiteLLM config with multiple providers:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",metastring:'title="litellm/config.yaml"',children:"model_list:\n  - model_name: gpt-4.1\n    litellm_params:\n      model: openai/gpt-4.1\n      api_key: os.environ/OPENAI_API_KEY\n\n  - model_name: claude-sonnet\n    litellm_params:\n      model: anthropic/claude-sonnet-4-5-20250929\n      api_key: os.environ/ANTHROPIC_API_KEY\n\n  - model_name: claude-haiku\n    litellm_params:\n      model: anthropic/claude-haiku-4-5-20251001\n      api_key: os.environ/ANTHROPIC_API_KEY\n"})}),"\n",(0,r.jsxs)(n.p,{children:["Dockerfile uses ",(0,r.jsx)(n.code,{children:"uv"})," for dependency management:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-dockerfile",metastring:'title="Dockerfile"',children:'FROM python:3.12-slim\nCOPY --from=ghcr.io/astral-sh/uv:latest /uv /uvx /bin/\nWORKDIR /code\nCOPY pyproject.toml .\nRUN uv sync --no-dev --no-install-project\nCOPY app/ app/\nCMD ["uv", "run", "uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]\n'})}),"\n",(0,r.jsx)(n.h3,{id:"database-schema",children:"Database Schema"}),"\n",(0,r.jsx)(n.p,{children:"E-commerce database -- customers, products, and ~200 orders spanning 6 months:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",metastring:'title="db/init.sql"',children:"CREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    email VARCHAR(150) UNIQUE NOT NULL,\n    segment VARCHAR(20) NOT NULL\n        CHECK (segment IN ('enterprise', 'mid-market', 'startup', 'consumer')),\n    region VARCHAR(30) NOT NULL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(150) NOT NULL,\n    category VARCHAR(50) NOT NULL,\n    price NUMERIC(10, 2) NOT NULL\n);\n\nCREATE TABLE orders (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    product_id INTEGER REFERENCES products(id),\n    quantity INTEGER NOT NULL CHECK (quantity > 0),\n    total_amount NUMERIC(12, 2) NOT NULL,\n    ordered_at TIMESTAMP NOT NULL\n);\n"})}),"\n",(0,r.jsx)(n.h3,{id:"agent-state--langgraph-workflow",children:"Agent State + LangGraph Workflow"}),"\n",(0,r.jsx)(n.p,{children:"The state carries context across all nodes:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="app/agent/state.py"',children:'from typing import Annotated, TypedDict\nfrom langgraph.graph.message import add_messages\n\nclass AgentState(TypedDict):\n    messages: Annotated[list, add_messages]\n    intent: str  # "data_question" | "follow_up" | "general"\n    sql: str\n    query_results: list[dict]\n    analysis: str\n    conversation_id: str\n'})}),"\n",(0,r.jsx)(n.p,{children:"The graph wires everything together. The LLM client points at LiteLLM -- the agent has no knowledge of which provider is serving completions:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="app/agent/graph.py"',children:'from functools import partial\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import END, StateGraph\nfrom app.agent.nodes import (\n    analyst_node, query_executor_node, responder_node,\n    router_node, sql_generator_node,\n)\nfrom app.agent.state import AgentState\nfrom app.config import settings\n\n\ndef _route_by_intent(state: AgentState) -> str:\n    intent = state.get("intent", "general")\n    if intent == "data_question":\n        return "sql_generator"\n    if intent == "follow_up":\n        return "analyst"\n    return "responder"\n\n\ndef build_graph(db_session, redis_client) -> StateGraph:\n    llm = ChatOpenAI(\n        base_url=settings.litellm_base_url,\n        api_key=settings.litellm_master_key,\n        model=settings.litellm_model,\n        temperature=0,\n    )\n    graph = StateGraph(AgentState)\n\n    graph.add_node("router", partial(router_node, llm=llm))\n    graph.add_node("sql_generator", partial(sql_generator_node, llm=llm))\n    graph.add_node("query_executor", partial(\n        query_executor_node, db_session=db_session, redis_client=redis_client,\n    ))\n    graph.add_node("analyst", partial(analyst_node, llm=llm))\n    graph.add_node("responder", partial(responder_node, llm=llm))\n\n    graph.set_entry_point("router")\n    graph.add_conditional_edges("router", _route_by_intent)\n    graph.add_edge("sql_generator", "query_executor")\n    graph.add_edge("query_executor", "analyst")\n    graph.add_edge("analyst", "responder")\n    graph.add_edge("responder", END)\n\n    return graph.compile()\n'})}),"\n",(0,r.jsx)(n.mermaid,{value:"graph TD\n    Start([Start]) --\x3e Router\n    Router --\x3e|data_question| SQLGen[SQL Generator]\n    Router --\x3e|follow_up| Analyst\n    Router --\x3e|general| Responder\n    SQLGen --\x3e Executor[Query Executor]\n    Executor --\x3e Analyst\n    Analyst --\x3e Responder\n    Responder --\x3e End([End])"}),"\n",(0,r.jsx)(n.h3,{id:"sql-tool",children:"SQL Tool"}),"\n",(0,r.jsx)(n.p,{children:"Read-only enforcement, keyword blocklist, and automatic row limits:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="app/tools/sql_tool.py"',children:'FORBIDDEN_PATTERN = re.compile(\n    r"\\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|TRUNCATE|GRANT|REVOKE|EXEC)\\b",\n    re.IGNORECASE,\n)\n\ndef _validate_sql(sql: str) -> str:\n    stripped = sql.strip().rstrip(";")\n    if not stripped.upper().startswith("SELECT"):\n        raise ValueError("Only SELECT queries are permitted.")\n    if FORBIDDEN_PATTERN.search(stripped):\n        raise ValueError("Query contains forbidden keywords.")\n    return stripped\n\nasync def query_database(sql, session, redis_client=None) -> list[dict]:\n    validated = _validate_sql(sql)\n\n    if redis_client:\n        key = f"sql_cache:{hashlib.sha256(validated.encode()).hexdigest()}"\n        cached = await redis_client.get(key)\n        if cached:\n            return json.loads(cached)\n\n    limited = f"{validated} LIMIT {settings.sql_row_limit}"\n    result = await session.execute(text(limited))\n    rows = [dict(row._mapping) for row in result.fetchall()]\n\n    if redis_client and rows:\n        await redis_client.setex(key, settings.query_cache_ttl, json.dumps(rows, default=str))\n    return rows\n'})}),"\n",(0,r.jsx)(n.h3,{id:"redis-memory",children:"Redis Memory"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="app/memory/redis_store.py"',children:'async def load_conversation(client, conversation_id: str) -> list[dict]:\n    data = await client.get(f"conversation:{conversation_id}")\n    return json.loads(data) if data else []\n\nasync def save_conversation(client, conversation_id: str, messages: list[dict]) -> None:\n    await client.setex(\n        f"conversation:{conversation_id}",\n        settings.conversation_ttl,\n        json.dumps(messages, default=str),\n    )\n'})}),"\n",(0,r.jsx)(n.h3,{id:"fastapi-routes",children:"FastAPI Routes"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="app/api/routes.py"',children:'@router.post("/chat", response_model=ChatResponse)\nasync def chat(\n    request: ChatRequest,\n    db_session=Depends(get_db_session),\n    redis_client=Depends(get_redis),\n):\n    history_data = await load_conversation(redis_client, request.conversation_id)\n    history = [\n        HumanMessage(content=m["content"]) if m["role"] == "human"\n        else AIMessage(content=m["content"])\n        for m in history_data\n    ]\n\n    agent = build_graph(db_session, redis_client)\n    result = await agent.ainvoke({\n        "messages": history + [HumanMessage(content=request.message)],\n        "intent": "", "sql": "", "query_results": [],\n        "analysis": "", "conversation_id": request.conversation_id,\n    })\n\n    all_messages = result["messages"]\n    await save_conversation(redis_client, request.conversation_id,\n        [{"role": "human" if isinstance(m, HumanMessage) else "ai", "content": m.content}\n         for m in all_messages])\n\n    ai_response = next(\n        (m.content for m in reversed(all_messages) if isinstance(m, AIMessage)),\n        "I could not generate a response.",\n    )\n    return ChatResponse(response=ai_response, conversation_id=request.conversation_id,\n                        sql=result.get("sql") or None)\n'})}),"\n",(0,r.jsx)(n.h3,{id:"mcp-server",children:"MCP Server"}),"\n",(0,r.jsx)(n.p,{children:"Expose tools for external clients:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",metastring:'title="app/mcp/server.py"',children:'@server.list_tools()\nasync def list_tools() -> list[Tool]:\n    return [\n        Tool(name="query_database",\n             description="Execute a read-only SELECT query against the e-commerce database.",\n             inputSchema={"type": "object",\n                          "properties": {"sql": {"type": "string"}},\n                          "required": ["sql"]}),\n        Tool(name="analyse_results",\n             description="Compute summary statistics over data rows.",\n             inputSchema={"type": "object",\n                          "properties": {"rows": {"type": "array", "items": {"type": "object"}}},\n                          "required": ["rows"]}),\n    ]\n'})}),"\n",(0,r.jsx)(n.p,{children:"Connect from Claude Desktop or Cursor:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{\n  "mcpServers": {\n    "data-agent": {\n      "command": "python",\n      "args": ["-m", "app.mcp.server"]\n    }\n  }\n}\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"running-it",children:"Running It"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/p-s-vishnu/data-agent.git\ncd data-agent\ncp .env.example .env\n# Add your LLM API key to .env\n\ndocker compose up --build\n"})}),"\n",(0,r.jsx)(n.p,{children:"Test:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/api/chat \\\n  -H "Content-Type: application/json" \\\n  -d \'{"message": "What are the top 5 products by revenue?", "conversation_id": "demo-1"}\'\n'})}),"\n",(0,r.jsx)(n.p,{children:"Follow-up (tests memory):"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'curl -X POST http://localhost:8000/api/chat \\\n  -H "Content-Type: application/json" \\\n  -d \'{"message": "Break that down by customer segment", "conversation_id": "demo-1"}\'\n'})}),"\n",(0,r.jsxs)(n.p,{children:["API docs at ",(0,r.jsx)(n.a,{href:"http://localhost:8000/docs",children:"http://localhost:8000/docs"}),"."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.anthropic.com/engineering/building-effective-agents",children:"Anthropic -- Building effective agents"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/NirDiamant/agents-towards-production",children:"NirDiamant -- Agents towards production"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://langchain-ai.github.io/langgraph/",children:"LangGraph documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.litellm.ai/",children:"LiteLLM documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://modelcontextprotocol.io/",children:"Model Context Protocol"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://fastapi.tiangolo.com/",children:"FastAPI documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://github.com/p-s-vishnu/data-agent",children:"Code: Companion code"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},4744(e,n,t){t.d(n,{A:()=>s});const s=t.p+"assets/images/agent_autonomy_levels-8d3d9da62bf1f7741fa7787a60a98743.png"},56(e,n,t){t.d(n,{R:()=>i,x:()=>o});var s=t(758);const r={},a=s.createContext(r);function i(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:i(e.components),s.createElement(a.Provider,{value:n},e.children)}},2748(e){e.exports=JSON.parse('{"permalink":"/building-a-data-analysis-agent","editUrl":"https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2025-08-03-data-agent.md","source":"@site/blog/2025-08-03-data-agent.md","title":"Building a Production Data Analysis Agent","description":"Build a data analysis agent that turns natural language into SQL, runs queries against a database, and returns insights -- using LangGraph, FastAPI, Redis, LiteLLM, and MCP. Everything runs locally via Docker Compose.","date":"2025-08-03T00:00:00.000Z","tags":[{"inline":false,"label":"Agents","permalink":"/tags/agents","description":"Multi-turn AI workflows, planning agents etc."},{"inline":false,"label":"LLM","permalink":"/tags/llm","description":"LLM, LLMs, etc."},{"inline":false,"label":"Software Engineering","permalink":"/tags/software-engineering","description":"Software engineering, design patterns, etc."}],"readingTime":8,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"building-a-data-analysis-agent","title":"Building a Production Data Analysis Agent","tags":["agents","llm","software-engineering"]},"unlisted":false,"nextItem":{"title":"Learnings from Monzo: AWS reInvent A Deep Dive into Building a Digital Bank","permalink":"/2024/01/06/Monzo-AWS-reInvent"}}')}}]);