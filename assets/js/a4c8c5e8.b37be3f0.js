"use strict";(globalThis.webpackChunkportfolio=globalThis.webpackChunkportfolio||[]).push([[7262],{7402(e,t,s){s.r(t),s.d(t,{assets:()=>o,contentTitle:()=>a,default:()=>c,frontMatter:()=>l,metadata:()=>n,toc:()=>d});var n=s(2963),i=s(6070),r=s(56);const l={slug:"why-your-llm-is-slow",title:"Why Your LLM Is Slow (And the 5 Papers That Fix It)",tags:["llm","mlops"]},a=void 0,o={authorsImageUrls:[]},d=[{value:"TLDR",id:"tldr",level:2},{value:"1. FlashAttention - Compute Optimisation",id:"1-flashattention---compute-optimisation",level:2},{value:"2. PagedAttention - Memory Management",id:"2-pagedattention---memory-management",level:2},{value:"3. Speculative Decoding - Faster Token Generation",id:"3-speculative-decoding---faster-token-generation",level:2},{value:"4. Heterogeneous GPU Serving - Cost Optimisation",id:"4-heterogeneous-gpu-serving---cost-optimisation",level:2},{value:"5. DistServe - Disaggregated Inference",id:"5-distserve---disaggregated-inference",level:2},{value:"References",id:"references",level:2}];function h(e){const t={a:"a",h2:"h2",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(t.p,{children:"Recently came across a post explaining these papers and thought it worth sharing with a quick breakdown."}),"\n",(0,i.jsx)(t.h2,{id:"tldr",children:"TLDR"}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Concept"}),(0,i.jsx)(t.th,{children:"Layer"}),(0,i.jsx)(t.th,{children:"Key Win"}),(0,i.jsx)(t.th,{children:"Remember This"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"FlashAttention"}),(0,i.jsx)(t.td,{children:"Compute"}),(0,i.jsx)(t.td,{children:"2-6x attention speedup"}),(0,i.jsx)(t.td,{children:"Tiling + IO-awareness"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"PagedAttention"}),(0,i.jsx)(t.td,{children:"Memory"}),(0,i.jsx)(t.td,{children:"Less than 4% waste (was 60-80%)"}),(0,i.jsx)(t.td,{children:"Virtual memory for KV cache"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Speculative Decoding"}),(0,i.jsx)(t.td,{children:"Generation"}),(0,i.jsx)(t.td,{children:"2-3.6x faster decoding"}),(0,i.jsx)(t.td,{children:"Draft-then-verify"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Heterogeneous Serving"}),(0,i.jsx)(t.td,{children:"Infrastructure"}),(0,i.jsx)(t.td,{children:"Up to 77% cost savings"}),(0,i.jsx)(t.td,{children:"Right GPU for right job"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"DistServe"}),(0,i.jsx)(t.td,{children:"Architecture"}),(0,i.jsx)(t.td,{children:"7.4x more requests"}),(0,i.jsx)(t.td,{children:"Split prefill from decoding"})]})]})]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"1-flashattention---compute-optimisation",children:"1. FlashAttention - Compute Optimisation"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The problem:"})," Attention is slow not because of maths, but because of memory traffic. GPUs have fast on-chip memory (SRAM, ~19TB/s) and slow main memory (HBM, ~2TB/s). Standard attention keeps shuffling data between them."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"How it works:"}),' Instead of computing the full N\xd7N attention matrix at once, FlashAttention tiles it into small blocks that fit in fast SRAM. It uses an "online softmax" trick to get exact results incrementally - no approximation.']}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Analogy:"})," Instead of carrying all your groceries inside in one impossible armful, you make smart small trips - but you planned the route so well it's actually faster."]}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Version"}),(0,i.jsx)(t.th,{children:"Speedup"}),(0,i.jsx)(t.th,{children:"GPU Utilisation"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"v1"}),(0,i.jsx)(t.td,{children:"2-3x vs standard"}),(0,i.jsx)(t.td,{children:"25-40%"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"v2"}),(0,i.jsx)(t.td,{children:"2x on top of v1"}),(0,i.jsx)(t.td,{children:"50-73%"})]})]})]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"2-pagedattention---memory-management",children:"2. PagedAttention - Memory Management"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The problem:"}),' Each request stores a KV cache (the model\'s "memory" of past tokens). Traditional systems pre-allocate memory for the worst case, wasting 60-80% of GPU memory. This limits how many requests you can batch together.']}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"How it works:"})," Borrows the virtual memory paging concept from operating systems. KV cache is split into fixed-size blocks that can be scattered anywhere in GPU memory. A block table maps logical to physical locations. Memory is allocated on-demand as tokens are generated."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Analogy:"})," Instead of reserving an entire bookshelf per person (wasteful), you let people's books sit on any available shelf and give them a card catalogue to find them."]}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Metric"}),(0,i.jsx)(t.th,{children:"Result"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Memory waste"}),(0,i.jsx)(t.td,{children:"60-80% to less than 4%"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Throughput vs HuggingFace"}),(0,i.jsx)(t.td,{children:"24x"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Throughput vs prior SOTA"}),(0,i.jsx)(t.td,{children:"2-4x"})]})]})]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Why it matters:"})," Memory efficiency means larger batch sizes, more requests per GPU, and lower cost. This is why vLLM became the industry standard for LLM serving."]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"3-speculative-decoding---faster-token-generation",children:"3. Speculative Decoding - Faster Token Generation"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The problem:"})," LLMs generate tokens one at a time, each requiring a full forward pass through billions of parameters. The GPU is massively underutilised - like hiring 1,000 workers to carry one brick at a time."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Two approaches:"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Speculative Sampling (DeepMind):"}),' A small fast "draft" model guesses the next k tokens. The big model verifies all k in one forward pass. If the guesses match, you got k tokens for the price of roughly 1. Mathematically guaranteed to produce identical output.']}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Medusa (Cai et al., Princeton/UIUC):"}),' Instead of a separate model, bolt extra "prediction heads" onto the main model. Each head predicts future tokens in parallel. Simpler deployment (one model), but requires fine-tuning.']}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Approach"}),(0,i.jsx)(t.th,{children:"Speedup"}),(0,i.jsx)(t.th,{children:"Trade-off"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Speculative Sampling"}),(0,i.jsx)(t.td,{children:"2-2.5x"}),(0,i.jsx)(t.td,{children:"Need two models"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Medusa-2"}),(0,i.jsx)(t.td,{children:"2.3-3.6x"}),(0,i.jsx)(t.td,{children:"Need to fine-tune heads"})]})]})]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Analogy:"})," Instead of asking the CEO to write a memo word by word, have an intern draft 5 sentences, then the CEO reviews them all at once - keeping what's good, rewriting what's not."]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"4-heterogeneous-gpu-serving---cost-optimisation",children:"4. Heterogeneous GPU Serving - Cost Optimisation"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The problem:"})," Companies buy expensive A100s for everything, but not all requests need top-tier hardware. Short chat messages don't need the same GPU as processing 100-page documents."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsx)(t.strong,{children:"Two approaches:"})}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Metis (training-focused):"})," Automatically figures out how to split training across mixed GPU types with smart load balancing. Result: 1-8.4x speedup."]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Melange (inference-focused):"})," Formulates GPU selection as a bin-packing problem - which mix of cheap and expensive GPUs minimises cost while meeting latency targets?"]}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Workload"}),(0,i.jsx)(t.th,{children:"Cost Savings"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Short chat"}),(0,i.jsx)(t.td,{children:"up to 77%"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Long documents"}),(0,i.jsx)(t.td,{children:"33%"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Mixed"}),(0,i.jsx)(t.td,{children:"51%"})]})]})]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Analogy:"})," Instead of sending limousines for every taxi ride, dispatch the right vehicle for each trip - sedans for solo riders, vans for groups."]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"5-distserve---disaggregated-inference",children:"5. DistServe - Disaggregated Inference"}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"The problem:"})," LLM inference has two phases with opposite needs:"]}),"\n",(0,i.jsxs)(t.ul,{children:["\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Prefill"})," (process the prompt): wants massive parallelism, high throughput"]}),"\n",(0,i.jsxs)(t.li,{children:[(0,i.jsx)(t.strong,{children:"Decoding"})," (generate tokens): wants low latency, small batches"]}),"\n"]}),"\n",(0,i.jsx)(t.p,{children:"Running both on the same GPU is like asking one chef to do both bulk meal prep and delicate plating simultaneously - neither goes well."}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"How it works:"})," Physically separate prefill and decoding onto different GPU clusters, each tuned for its workload. After prefill generates the KV cache, it is shipped to the decoding cluster."]}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{children:"Metric"}),(0,i.jsx)(t.th,{children:"Result"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"Goodput vs vLLM (requests meeting SLO/s)"}),(0,i.jsx)(t.td,{children:"7.4x more"})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{children:"SLO compliance"}),(0,i.jsx)(t.td,{children:"Over 90% of requests meet latency targets"})]})]})]}),"\n",(0,i.jsxs)(t.p,{children:[(0,i.jsx)(t.strong,{children:"Analogy:"})," A restaurant with a separate prep kitchen (high-volume chopping) and a plating station (precision finishing), connected by a runner."]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h2,{id:"references",children:"References"}),"\n",(0,i.jsxs)(t.ol,{children:["\n",(0,i.jsxs)(t.li,{children:["Dao et al. (2022) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2205.14135",children:"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"})]}),"\n",(0,i.jsxs)(t.li,{children:["Dao (2023) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2307.08691",children:"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning"})]}),"\n",(0,i.jsxs)(t.li,{children:["Kwon et al. (2023) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2309.06852",children:"Efficient Memory Management for Large Language Model Serving with PagedAttention"})]}),"\n",(0,i.jsxs)(t.li,{children:["Chen et al. (2023) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2211.17192",children:"Accelerating Large Language Model Decoding with Speculative Sampling"})]}),"\n",(0,i.jsxs)(t.li,{children:["Cai et al. (2024) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2401.02659",children:"Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads"})]}),"\n",(0,i.jsxs)(t.li,{children:["Um et al. (2022) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2208.14226",children:"Metis: Fast Automatic Distributed Training on Heterogeneous GPUs"})]}),"\n",(0,i.jsxs)(t.li,{children:["Griggs et al. (2024) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2404.14527",children:"M\xe9lange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity"})]}),"\n",(0,i.jsxs)(t.li,{children:["Zhong et al. (2024) : ",(0,i.jsx)(t.a,{href:"https://arxiv.org/abs/2401.09670",children:"DistServe: Disaggregating Prefill and Decoding for Goodput-Optimized LLM Serving"})]}),"\n"]})]})}function c(e={}){const{wrapper:t}={...(0,r.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},56(e,t,s){s.d(t,{R:()=>l,x:()=>a});var n=s(758);const i={},r=n.createContext(i);function l(e){const t=n.useContext(r);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),n.createElement(r.Provider,{value:t},e.children)}},2963(e){e.exports=JSON.parse('{"permalink":"/why-your-llm-is-slow","editUrl":"https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2026-02-22-why-your-llm-is-slow.md","source":"@site/blog/2026-02-22-why-your-llm-is-slow.md","title":"Why Your LLM Is Slow (And the 5 Papers That Fix It)","description":"Recently came across a post explaining these papers and thought it worth sharing with a quick breakdown.","date":"2026-02-22T00:00:00.000Z","tags":[{"inline":false,"label":"LLM","permalink":"/tags/llm","description":"LLM, LLMs, etc."},{"inline":false,"label":"MLOps","permalink":"/tags/mlops","description":"MLOps, end to end lifecycle"}],"readingTime":4.67,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"why-your-llm-is-slow","title":"Why Your LLM Is Slow (And the 5 Papers That Fix It)","tags":["llm","mlops"]},"unlisted":false,"nextItem":{"title":"Building a Production Data Analysis Agent","permalink":"/building-a-data-analysis-agent"}}')}}]);