"use strict";(self.webpackChunkportfolio=self.webpackChunkportfolio||[]).push([[9575],{767:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vertexai-pipeline-8b601ffe899e82f7c31009d43d0f9b82.png"},2346:e=>{e.exports=JSON.parse('{"permalink":"/online-prediction-using-gcp-vertex-ai","editUrl":"https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/blog/2022-05-22-online-prediction-using-gcp-vertex-ai.md","source":"@site/blog/2022-05-22-online-prediction-using-gcp-vertex-ai.md","title":"Online prediction using GCP\u2019s Vertex AI","description":"Serve and process real-time data with a Tensorflow model using Pub-Sub, Cloud Dataflow, BigQuery and Vertex AI.","date":"2022-05-22T00:00:00.000Z","tags":[{"inline":false,"label":"Use Case","permalink":"/tags/usecase","description":"Company Use Cases, whitepaper, product, etc."},{"inline":false,"label":"Machine Learning","permalink":"/tags/machine-learning","description":"Classical Machine Learning"}],"readingTime":8.64,"hasTruncateMarker":true,"authors":[],"frontMatter":{"slug":"online-prediction-using-gcp-vertex-ai","title":"Online prediction using GCP\u2019s Vertex AI","tags":["usecase","machine-learning"]},"unlisted":false,"prevItem":{"title":"Learnings from Monzo: AWS reInvent A Deep Dive into Building a Digital Bank","permalink":"/2024/01/06/Monzo-AWS-reInvent"},"nextItem":{"title":"Common Product Metrics","permalink":"/product-metrics"}}')},7295:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>d,frontMatter:()=>r,metadata:()=>t,toc:()=>p});var t=i(2346),o=i(4848),a=i(8453);const r={slug:"online-prediction-using-gcp-vertex-ai",title:"Online prediction using GCP\u2019s Vertex AI",tags:["usecase","machine-learning"]},s="Online prediction using GCP\u2019s Vertex AI",l={authorsImageUrls:[]},p=[{value:"Table of Content",id:"table-of-content",level:2},{value:"1. Architecture",id:"1-architecture",level:2},{value:"2. Data Ingestion",id:"2-data-ingestion",level:2},{value:"Pub-Sub",id:"pub-sub",level:3},{value:"Big Query",id:"big-query",level:3},{value:"Dataflow",id:"dataflow",level:3},{value:"3. Model Deployment",id:"3-model-deployment",level:2},{value:"Vertex AI",id:"vertex-ai",level:3},{value:"Kubeflow pipeline",id:"kubeflow-pipeline",level:3},{value:"4. Prediction",id:"4-prediction",level:2}];function c(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.p,{children:"Serve and process real-time data with a Tensorflow model using Pub-Sub, Cloud Dataflow, BigQuery and Vertex AI."}),"\n",(0,o.jsxs)(n.p,{children:["In this project, we are predicting the travel fare when a user books a cab. Unlike traditional pricing calculation, here the price is calculated dynamically based on multiple parameters, including the feature ",(0,o.jsx)(n.strong,{children:"number of trips in the last 5 minutes"}),", which acts as a proxy for real-time traffic."]}),"\n",(0,o.jsx)(n.h2,{id:"table-of-content",children:"Table of Content"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Architecture"}),"\n",(0,o.jsx)(n.li,{children:"Data Ingestion"}),"\n",(0,o.jsx)(n.li,{children:"Model Deployment"}),"\n",(0,o.jsx)(n.li,{children:"Prediction"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"1-architecture",children:"1. Architecture"}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"GCP\u2019s ASL repository",src:i(8356).A+"",width:"1400",height:"847"})}),"\n",(0,o.jsx)(n.p,{children:"The whole system can be divided into two parts"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Ingesting continuous feed of taxi trip data"}),"\n",(0,o.jsx)(n.li,{children:"Predicting on-demand fare requests"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"2-data-ingestion",children:"2. Data Ingestion"}),"\n",(0,o.jsx)(n.p,{children:"For the data ingestion part, we will need Pub/Sub, Dataflow and Big query. Let's configure the required services one by one."}),"\n",(0,o.jsx)(n.h3,{id:"pub-sub",children:"Pub-Sub"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Pub/Sub is used for streaming analytics and data integration pipelines to ingest and distribute data. It is equally effective as a messaging-oriented middleware for service integration or as a queue to parallelize tasks. ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/pubsub/docs/overview#core_concepts",children:"Read more about the core concepts like - topics, subscription, publisher, etc."})]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Here Pub/Sub will be used as a messaging bus that receives and stores recently completed taxi trips."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="pubsub.py"',children:'import logging\nfrom google import api_core\nfrom google.cloud import pubsub_v1 as pubsub\n\n\ndef get_pubsub_client(gcp_project_id, topic="taxi_rides"):\n    """ Get topic if already exists or else create a new one\n    """\n    publisher = pubsub.PublisherClient()\n    topic_name = publisher.topic_path(gcp_project_id, topic)\n    try:\n        publisher.get_topic(topic_name)\n        logging.info("Reusing pub/sub topic %s", topic)\n    except api_core.exceptions.NotFound:\n        publisher.create_topic(topic_name)\n        logging.info("Creating pub/sub topic %s", topic)\n    return publisher\n'})}),"\n",(0,o.jsx)(n.p,{children:"Since we cannot get live taxi trip data we will create a python script that randomly generates trip data and pushes it to the Pub/Sub."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:"PROJECT_ID=$(gcloud config get-value project)\nREGION=$(gcloud config get-value ai/region)\nBUCKET=$PROJECT_ID # change as necessary\n"})}),"\n",(0,o.jsx)(n.p,{children:"Run the following code as a separate script, it is configured to send about 2,000 trip messages every five minutes with some randomness in the frequency to mimic traffic fluctuations. These numbers come from looking at the historical average of taxi ride frequency in BigQuery."}),"\n",(0,o.jsx)(n.p,{children:"In production this script would be replaced with actual taxis with IoT devices sending trip data to Cloud Pub/Sub."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'publisher = get_pubsub_client(PROJECT_ID, topic="taxi_rides")\nwhile True:\n  num_trips = random.randint(10, 60)\n  for i in range(num_trips):\n    publisher.publish(topic_name, b"taxi_ride")\n  logging.info("Publishing: %s", time.ctime())\n  time.sleep(5)\n'})}),"\n",(0,o.jsx)(n.h3,{id:"big-query",children:"Big Query"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:["Big Query is a Data warehouse managed by google to store, process, analyse, and visualize large amounts of data. ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/bigquery/docs/introduction",children:"Link to read more"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="bigquery.py"',children:'import logging\nfrom google import api_core\nfrom google.cloud import bigquery\n\n\ndef create_dataset(dataset_id="taxifare"):\n    bq = bigquery.Client()\n    dataset = bigquery.Dataset(bq.dataset(dataset_id))\n    try:\n        bq.create_dataset(dataset)  # will fail if dataset already exists\n        logging.info("Dataset created.")\n    except api_core.exceptions.Conflict:\n        logging.info("Dataset already exists.")\n\ndef create_table(dataset_id="taxifare", table_name="traffic_realtime"):\n    bq = bigquery.Client()\n    dataset = bigquery.Dataset(bq.dataset(dataset_id))\n    table_ref = dataset.table(table_name)\n    SCHEMA = [\n        bigquery.SchemaField("trips_last_5min", "INTEGER", mode="REQUIRED"),\n        bigquery.SchemaField("time", "TIMESTAMP", mode="REQUIRED"),\n    ]\n    table = bigquery.Table(table_ref, schema=SCHEMA)\n    try:\n        bq.create_table(table)\n        logging.info("Table created.")\n    except api_core.exceptions.Conflict:\n        logging.info("Table already exists.")\n'})}),"\n",(0,o.jsx)(n.h3,{id:"dataflow",children:"Dataflow"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=XdsuDOQ9nkU&ab_channel=GoogleCloudTech",children:"Dataflow"})," is a fast, ",(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=vxJobGtqKVM&ab_channel=IBMTechnology",children:"serverless"})," service for executing batch and streaming data processing pipelines. You create your pipelines with Apache Beam and then run them using the Dataflow service.\n",(0,o.jsx)(n.a,{href:"https://beam.apache.org/documentation/basics/",children:"Link to basics of Apache beam"})," & ",(0,o.jsx)(n.a,{href:"https://cloud.google.com/dataflow",children:"Link to read more about Dataflow"}),"."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Now that our taxi data is pushed to Pub/Sub, and our BigQuery table is set up, let\u2019s consume the Pub/Sub data using a streaming DataFlow pipeline."}),"\n",(0,o.jsx)(n.p,{children:"Dataflow will be responsible for the following transformations:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Pull the completed trips from the Pub/Sub"}),"\n",(0,o.jsx)(n.li,{children:"Window the messages (every 5 mins)"}),"\n",(0,o.jsx)(n.li,{children:"Count the number of messages in the window"}),"\n",(0,o.jsx)(n.li,{children:"Format the count for BigQuery"}),"\n",(0,o.jsx)(n.li,{children:"Write results to BigQuery table"}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="dataflow.py"',children:'import argparse\nfrom datetime import datetime\n\nimport apache_beam as beam\nfrom apache_beam.options.pipeline_options import (\n    GoogleCloudOptions,\n    PipelineOptions,\n    SetupOptions,\n    StandardOptions,\n)\nfrom apache_beam.transforms import window  # pylint: disable=unused-import\n\n\nclass CountFn(beam.CombineFn):\n    """Counter function to accumulate statistics"""\n    def create_accumulator(self):\n        return 0\n\n    def add_input(self, count, element):\n        del element\n        return count + 1\n\n    def merge_accumulators(self, accumulators):\n        return sum(accumulators)\n\n    def extract_output(self, count):\n        return count\n\n\ndef run(argv=None):\n    """Build and run the pipeline."""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        "--project", help=("Google Cloud Project ID"), required=True\n    )\n    parser.add_argument("--region", help=("Google Cloud region"), required=True)\n    parser.add_argument(\n        "--input_topic",\n        help=("Google Cloud PubSub topic name "),\n        required=True,\n    )\n    known_args, pipeline_args = parser.parse_known_args(argv)\n    pipeline_options = PipelineOptions(pipeline_args)\n    pipeline_options.view_as(SetupOptions).save_main_session = True\n    pipeline_options.view_as(StandardOptions).streaming = True\n    pipeline_options.view_as(GoogleCloudOptions).region = known_args.region\n    pipeline_options.view_as(GoogleCloudOptions).project = known_args.project\n    p = beam.Pipeline(options=pipeline_options)\n    \n    topic = f"projects/{known_args.project}/topics/{known_args.input_topic}"\n    # this table needs to exist\n    table_spec = f"{known_args.project}:taxifare.traffic_realtime"\n\n    def to_bq_format(count):\n        """BigQuery writer requires rows to be stored as python dictionary"""\n        return {\n            "trips_last_5min": count,\n            "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),\n        }\n    pipeline = (  # noqa F841 pylint: disable=unused-variable\n        p\n        | "read_from_pubsub"\n        >> beam.io.ReadFromPubSub(topic=topic).with_output_types(bytes)\n        | "window"\n        >> beam.WindowInto(window.SlidingWindows(size=300, period=15))\n        | "count" >> beam.CombineGlobally(CountFn()).without_defaults()\n        | "format_for_bq" >> beam.Map(to_bq_format)\n        | "write_to_bq"\n        >> beam.io.WriteToBigQuery(\n            table_spec,\n            # WRITE_TRUNCATE not supported for streaming\n            write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND,\n            create_disposition=beam.io.BigQueryDisposition.CREATE_NEVER,\n        )\n    )\n    # result.wait_until_finish() #only do this if running with DirectRunner\n    result = p.run()  # noqa F841 pylint: disable=unused-variable\n'})}),"\n",(0,o.jsx)(n.p,{children:"Launch the dataflow pipeline using the command below."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-shell",children:"python3 dataflow.py --input_topic taxi_rides --runner=DataflowRunner --project=$PROJECT_ID --region=$REGION --temp_location=gs://$BUCKET/dataflow_streaming\n"})}),"\n",(0,o.jsx)(n.h2,{id:"3-model-deployment",children:"3. Model Deployment"}),"\n",(0,o.jsx)(n.h3,{id:"vertex-ai",children:"Vertex AI"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=gT4qqHMiEpA&ab_channel=GoogleCloudTech",children:"Vertex AI"}),"\xa0is a Jupyter-based fully managed, scalable, enterprise-ready compute infrastructure with security controls and user management capabilities. It serves as a one-stop environment to complete all of the ML work, from experimentation to deployment, to managing and monitoring models.\xa0",(0,o.jsx)(n.a,{href:"https://cloud.google.com/vertex-ai",children:"Link to read more"})]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"For keeping the article short, I have refrained from explaining the training code. The code can be found in the below GitHub repo. Do let me know in the comments if an explanation of the training code would help."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Vertex AI Pipeline",src:i(767).A+"",width:"1240",height:"1034"})}),"\n",(0,o.jsx)(n.h3,{id:"kubeflow-pipeline",children:"Kubeflow pipeline"}),"\n",(0,o.jsxs)(n.blockquote,{children:["\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.a,{href:"https://www.youtube.com/watch?v=cTZArDgbIWw&ab_channel=GoogleCloudTech",children:"Kubeflow"}),"\xa0is known as the ML toolkit for\xa0",(0,o.jsx)(n.a,{href:"https://kubernetes.io/",children:"Kubernetes"}),". The project is dedicated to making deployments of Machine Learning (ML) workflows on Kubernetes simple, portable, and scalable. The goal is to provide a straightforward way to deploy best-of-breed open-source systems for ML in diverse infrastructures.",(0,o.jsx)(n.br,{}),"\n",(0,o.jsx)(n.a,{href:"https://www.kubeflow.org/docs/started/architecture/",children:"Link to read more"})]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"We will make our prediction service available now. For that, we will be wrapping the following processes as a kubeflow pipeline."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Model training:"}),"\xa0Based on the latest labelled data start the model training as a serverless job."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Upload model:"}),"\xa0The output of the trained model will be transferred to Google cloud storage."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Create endpoint:"}),"\xa0While the model is being trained create an endpoint for the same asynchronously."]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Deployment:"}),"\xa0Finally deploy the model to the created endpoint."]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="train_upload_deploy_pipeline.py"',children:'from kfp.v2.dsl import component, pipeline\nfrom kfp.v2.google import experimental\nfrom google_cloud_pipeline_components.aiplatform import ModelUploadOp, EndpointCreateOp, ModelDeployOp\n\n\n@component\ndef training_op(input1: str):\n    print(f"VertexAI pipeline: {input1}")\n\n@pipeline(name="taxifare--train-upload-endpoint-deploy")\ndef pipeline(\n    project: str = PROJECT,\n    model_display_name: str = MODEL_DISPLAY_NAME,\n):\n    # 1. Model Training\n    train_task = training_op("taxifare training pipeline")\n    experimental.run_as_aiplatform_custom_job(\n        train_task,\n        display_name=f"pipelines-train-{TIMESTAMP}",\n        worker_pool_specs=[\n            {\n                "pythonPackageSpec": {\n                    "executor_image_uri": PYTHON_PACKAGE_EXECUTOR_IMAGE_URI,\n                    "package_uris": [PYTHON_PACKAGE_URIS],\n                    "python_module": PYTHON_MODULE,\n                    "args": [\n                        f"--eval_data_path={EVAL_DATA_PATH}",\n                        f"--output_dir={OUTDIR}",\n                        f"--train_data_path={TRAIN_DATA_PATH}",\n                        f"--batch_size={BATCH_SIZE}",\n                        f"--num_examples_to_train_on={NUM_EXAMPLES_TO_TRAIN_ON}",  # noqa: E501\n                        f"--num_evals={NUM_EVALS}",\n                        f"--nbuckets={NBUCKETS}",\n                        f"--lr={LR}",\n                        f"--nnsize={NNSIZE}",\n                    ],\n                },\n                "replica_count": f"{REPLICA_COUNT}",\n                "machineSpec": {\n                    "machineType": f"{MACHINE_TYPE}",\n                },\n            }\n        ],\n    )\n\n    # 2. Model Upload\n    model_upload_op = ModelUploadOp(\n        project=f"{PROJECT}",\n        display_name=f"pipelines-ModelUpload-{TIMESTAMP}",\n        artifact_uri=f"{OUTDIR}/savedmodel",\n        serving_container_image_uri=f"{SERVING_CONTAINER_IMAGE_URI}",\n        serving_container_environment_variables={"NOT_USED": "NO_VALUE"},\n    )\n    model_upload_op.after(train_task)\n\n    # 3. Create Endpoint\n    endpoint_create_op = EndpointCreateOp(\n        project=f"{PROJECT}",\n        display_name=f"pipelines-EndpointCreate-{TIMESTAMP}",\n    )\n\n    # 4. Deployment\n    model_deploy_op = ModelDeployOp(\n        project=f"{PROJECT}",\n        endpoint=endpoint_create_op.outputs["endpoint"],\n        model=model_upload_op.outputs["model"],\n        deployed_model_display_name=f"{MODEL_DISPLAY_NAME}",\n        machine_type=f"{MACHINE_TYPE}",\n    )\n    \n# Compile the pipeline\nfrom kfp.v2 import compiler\n\nif not os.path.isdir("vertex_pipelines"):\n    os.mkdir("vertex_pipelines")\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline,\n    package_path="./vertex_pipelines/train_upload_endpoint_deploy.json",\n'})}),"\n",(0,o.jsx)(n.p,{children:"If the compilation is successful then you can run the following code and your pipeline will start running."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="run_kfpipeline.py"',children:'# Run the pipeline\nfrom google_cloud_pipeline_components import aiplatform\n\npipeline_job = aiplatform.pipeline_jobs.PipelineJob(\n    display_name="taxifare_pipeline",\n    template_path="./vertex_pipelines/train_upload_endpoint_deploy.json",\n    pipeline_root=f"{PIPELINE_ROOT}",\n    project=PROJECT,\n    location=REGION,\n)\npipeline_job.run()\n'})}),"\n",(0,o.jsx)(n.p,{children:"Once the pipeline starts, head to Vertex AI > Pipeline and you should be able to see the pipeline similar to the below screenshot."}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.img,{alt:"Vertex AI Pipeline",src:i(767).A+"",width:"1240",height:"1034"})}),"\n",(0,o.jsx)(n.h2,{id:"4-prediction",children:"4. Prediction"}),"\n",(0,o.jsx)(n.p,{children:"From the previous step, save the endpoint(ENDPOINT) where our model is deployed. We need only two more components to make this system complete."}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"A function that would fetch the last 5 min traffic and add it as a feature to the request."}),"\n",(0,o.jsx)(n.li,{children:"Another function is to pass on the modified request, create the prediction service client and present the result."}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",metastring:'title="fare_prediction.py"',children:'import logging\nfrom typing import Dict, List, Union\n\nfrom google.cloud import aiplatform, bigquery\nfrom google.protobuf import json_format\nfrom google.protobuf.struct_pb2 import Value\n\ndef add_traffic_last_5min(instance, dataset="taxifare", table="traffic_realtime"):\n    """ Adds the dynamic feature `traffic_last_5min` to the instance\n    """\n    bq = bigquery.Client()\n    query_string = f"""\n    SELECT\n      *\n    FROM\n      `{dataset}.{table}`\n    ORDER BY\n      time DESC\n    LIMIT 1\n    """\n    trips = bq.query(query_string).to_dataframe()["trips_last_5min"][0]\n    instance["traffic_last_5min"] = int(trips)\n    return instance\n\ndef predict(\n    project: str,\n    endpoint_id: str,\n    instances: Union[Dict, List[Dict]],\n    location: str = "us-central1",\n    api_endpoint: str = "us-central1-aiplatform.googleapis.com",\n):\n    """\n    `instances` can be either single instance of type dict or a list\n    of instances.\n    Reference: https://github.com/googleapis/python-aiplatform/blob/master/samples/snippets/predict_custom_trained_model_sample.py\n    """\n    client_options = {"api_endpoint": api_endpoint}     # f"{REGION}-aiplatform.googleapis.com"\n    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n    \n    instances = instances if type(instances) == list else [instances]\n    instances = [add_traffic_last_5min(instance_dict) for instance_dict in instances]\n    instances = [\n        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n    ]\n    parameters_dict = {}\n    parameters = json_format.ParseDict(parameters_dict, Value())\n    endpoint = client.endpoint_path(\n        project=project, location=location, endpoint=endpoint_id\n    )\n    response = client.predict(\n        endpoint=endpoint, instances=instances, parameters=parameters\n    )\n    logging.info("response")\n    logging.info(f" deployed_model_id: {response.deployed_model_id}")\n    \n    # The predictions are a google.protobuf.Value representation of the model\'s predictions.\n    predictions = response.predictions\n    for prediction in predictions:\n        logging.info(" prediction:", dict(prediction))\n'})}),"\n",(0,o.jsx)(n.p,{children:"Now you are ready to hit the endpoint and receive the prediction."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'instance = { \n    "dayofweek": 4, \n    "hourofday": 13, \n    "pickup_longitude":-73.99, \n    "pickup_latitude": 40.758, \n    "dropoff_latitude": 41.742, \n    "dropoff_longitude": -73.07\n}\npredict(PROJECT_ID, ENDPOINT, instance)\n\n'})}),"\n",(0,o.jsxs)(n.p,{children:["You can refer to my repository for the complete code: ",(0,o.jsx)(n.a,{href:"https://github.com/p-s-vishnu/taxibooking-vertexai",children:"\ud83d\udcbb Code"}),"."]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8356:(e,n,i)=>{i.d(n,{A:()=>t});const t=i.p+"assets/images/vertexai-architecture-a59a225fd77db70ed43c6565a61c3542.png"},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>s});var t=i(6540);const o={},a=t.createContext(o);function r(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);