<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-papers/why models hallucinate" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">Why Language Models Hallucinate | Vishnu P.S.</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://p-s-vishnu.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://p-s-vishnu.github.io/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://p-s-vishnu.github.io/notes/papers/why models hallucinate"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Why Language Models Hallucinate | Vishnu P.S."><meta data-rh="true" name="description" content="Source//arxiv.org/html/2509.04664v1"><meta data-rh="true" property="og:description" content="Source//arxiv.org/html/2509.04664v1"><link data-rh="true" rel="icon" href="/img/old/icon_minion.png"><link data-rh="true" rel="canonical" href="https://p-s-vishnu.github.io/notes/papers/why models hallucinate"><link data-rh="true" rel="alternate" href="https://p-s-vishnu.github.io/notes/papers/why models hallucinate" hreflang="en"><link data-rh="true" rel="alternate" href="https://p-s-vishnu.github.io/notes/papers/why models hallucinate" hreflang="x-default"><link data-rh="true" rel="preconnect" href="https://0T9WWE13Y1-dsn.algolia.net" crossorigin="anonymous"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Why Language Models Hallucinate","item":"https://p-s-vishnu.github.io/notes/papers/why models hallucinate"}]}</script><link rel="alternate" type="application/rss+xml" href="/rss.xml" title="Vishnu P.S. RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/atom.xml" title="Vishnu P.S. Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J8413B0NWF"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-J8413B0NWF",{anonymize_ip:!0})</script>



<link rel="search" type="application/opensearchdescription+xml" title="Vishnu P.S." href="/opensearch.xml"><link rel="stylesheet" href="/assets/css/styles.6c1da29c.css">
<script src="/assets/js/runtime~main.b95803ed.js" defer="defer"></script>
<script src="/assets/js/main.90f4e306.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/old/icon_minion.png" alt="Vishnu&#x27;s Portfolio Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/old/icon_minion.png" alt="Vishnu&#x27;s Portfolio Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">Vishnu P.S.</b></a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/notes">Notes</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/">Blog</a><a class="navbar__item navbar__link" href="/about">About</a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search (Command+K)"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20" aria-hidden="true"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"></span></button></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes/">Hi!</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/notes/Agents &amp; Workflows">Agents &amp; Workflows</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/notes/Languages/Python">Languages</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/notes/papers/why models hallucinate">papers</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/notes/papers/why models hallucinate">Why Language Models Hallucinate</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">papers</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">Why Language Models Hallucinate</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Why Language Models Hallucinate</h1></header><p>Source: Why Language Models Hallucinate <a href="https://arxiv.org/html/2509.04664v1" target="_blank" rel="noopener noreferrer">https://arxiv.org/html/2509.04664v1</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="tldr">TLDR<a href="#tldr" class="hash-link" aria-label="Direct link to TLDR" title="Direct link to TLDR">​</a></h3>
<ol>
<li>Language models (LLMs) hallucinate because they are trained and evaluated in a way that rewards guessing over admitting uncertainty.</li>
<li>The paper presents a theoretical argument showing that hallucinations are a natural statistical outcome of the pre-training process, not just a bug.</li>
<li>The core reason they persist is that most industry benchmarks use binary (right/wrong) scoring, which incentivises models to make a confident guess rather than say &quot;I don&#x27;t know.&quot;</li>
<li>The authors propose a socio-technical solution: modify existing evaluation benchmarks to include explicit penalties for incorrect answers, thereby encouraging models to only respond when genuinely confident.</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="technical-points-to-know">Technical Points to Know<a href="#technical-points-to-know" class="hash-link" aria-label="Direct link to Technical Points to Know" title="Direct link to Technical Points to Know">​</a></h3>
<p>The paper introduces several key technical concepts to formalise the origins of hallucinations:</p>
<ul>
<li>
<p><strong>Reduction to Binary Classification:</strong> The paper&#x27;s central technical innovation is reducing the unsupervised problem of generative error (hallucination) to a supervised binary classification problem, which they term <strong>&quot;Is-It-Valid&quot; (IIV)</strong>. This allows the application of decades of statistical learning theory to LLM errors.</p>
<ul>
<li>Any LLM <code>p̂</code> can be converted into a classifier <code>f̂</code> by thresholding its probability assignment: an output <code>x</code> is classified as valid (&#x27;+&#x27;) if <code>p̂(x)</code> is above a certain value, and an error (&#x27;-&#x27;) otherwise.</li>
<li>This leads to the core inequality: <strong><code>err ≥ 2 * err_iiv - (bias terms)</code></strong>, where <code>err</code> is the model&#x27;s generative error rate and <code>err_iiv</code> is its misclassification rate on the IIV problem. This mathematically links generative errors to classification errors.</li>
</ul>
</li>
<li>
<p><strong>Calibration and Cross-Entropy:</strong> The standard cross-entropy loss objective used in pretraining naturally produces models that are &quot;calibrated&quot; in a specific sense. The paper shows that for such calibrated models, errors are a statistical necessity. A model that never errs (e.g., by always saying &quot;I don&#x27;t know&quot;) would have to be poorly calibrated with respect to the cross-entropy objective, meaning it would not be a local minimum for the training loss.</p>
</li>
<li>
<p><strong>Arbitrary-Fact Hallucinations and Singletons:</strong> For facts that have no learnable pattern (e.g., a person&#x27;s birthday), the model&#x27;s ability to recall them depends on their frequency in the training data. The paper connects the hallucination rate to the &quot;singleton rate&quot; (<code>sr</code>), which is the fraction of training prompts that appear only once.</p>
<ul>
<li><strong>Theorem 2</strong> states that for these types of facts, the expected error rate is approximately equal to the singleton rate (<code>err ≈ sr</code>). If 20% of birthday facts in the training data are singletons, a base model is expected to hallucinate on at least 20% of birthday-related queries.</li>
</ul>
</li>
<li>
<p><strong>Poor Models and Agnostic Learning:</strong> Hallucinations can also arise when a model&#x27;s architecture is fundamentally unsuited for a task. The paper connects this to the concept of <code>opt(G)</code> from agnostic learning—the best possible error rate achievable by any classifier within a given family <code>G</code>. If <code>opt(G)</code> is high for a certain task, then any model from that family is guaranteed to have a high error rate.</p>
<ul>
<li>An example is a token-based LLM being asked to count characters, a task for which its representation may be ill-suited, whereas a model with a reasoning mechanism might perform better.</li>
</ul>
</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="important-sections">Important Sections<a href="#important-sections" class="hash-link" aria-label="Direct link to Important Sections" title="Direct link to Important Sections">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="pretraining-errors-section-3"><strong>Pretraining Errors (Section 3)</strong><a href="#pretraining-errors-section-3" class="hash-link" aria-label="Direct link to pretraining-errors-section-3" title="Direct link to pretraining-errors-section-3">​</a></h4>
<p>This section provides the theoretical foundation for why base models hallucinate before any fine-tuning.</p>
<ul>
<li><strong>Key Insight:</strong> Hallucinations are not a mysterious emergent property but a predictable consequence of statistical density estimation. Even with perfectly clean training data, the pressure to create a good probabilistic model of the data forces the model to generate errors on less-frequent or unlearnable patterns. The reduction to the IIV classification problem is the main tool used here to demonstrate that if a concept is hard to classify, it will be hard to generate correctly. The analysis shows that for a model to avoid errors entirely, it would have to perform poorly on the fundamental pretraining objective of density estimation.</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="why-hallucinations-survive-post-training-section-4"><strong>Why Hallucinations Survive Post-Training (Section 4)</strong><a href="#why-hallucinations-survive-post-training-section-4" class="hash-link" aria-label="Direct link to why-hallucinations-survive-post-training-section-4" title="Direct link to why-hallucinations-survive-post-training-section-4">​</a></h4>
<p>This section shifts from theoretical origins to a practical, socio-technical explanation for why methods like RLHF have not eliminated hallucinations.</p>
<ul>
<li><strong>Key Insight:</strong> The evaluation ecosystem is the primary culprit. Most influential benchmarks (e.g., MMLU-Pro, SWE-bench, MATH) use binary grading where answers are simply right or wrong. Under this scheme, a model that guesses when uncertain has a higher expected score than one that abstains by saying &quot;I don&#x27;t know.&quot; Since models are optimised to climb these leaderboards, they are implicitly trained to hallucinate [1]. The paper describes this as an &quot;epidemic&quot; of penalising uncertainty [1].</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="proposed-mitigation-explicit-confidence-targets-section-42"><strong>Proposed Mitigation: Explicit Confidence Targets (Section 4.2)</strong><a href="#proposed-mitigation-explicit-confidence-targets-section-42" class="hash-link" aria-label="Direct link to proposed-mitigation-explicit-confidence-targets-section-42" title="Direct link to proposed-mitigation-explicit-confidence-targets-section-42">​</a></h4>
<p>This section offers a concrete, actionable solution aimed at the entire ML community rather than a specific model-based fix.</p>
<ul>
<li>
<p><strong>Key Insight:</strong> To build more trustworthy models, the rules of the game must change. Instead of creating more specialised (and often ignored) hallucination benchmarks, the authors propose modifying the primary benchmarks themselves. They suggest adding instructions to prompts that specify a confidence threshold <code>t</code> and an associated penalty for wrong answers. For example:</p>
<blockquote>
<p>&quot;Answer only if you are &gt;90% confident. Correct answers get 1 point, &#x27;I don&#x27;t know&#x27; gets 0 points, and incorrect answers get -9 points.&quot;</p>
</blockquote>
</li>
<li>
<p>This makes the optimal strategy for the model one of &quot;behavioural calibration&quot;—it should only answer if its internal confidence is higher than the stated threshold <code>t</code>. By making the threshold explicit in the prompt, a single model can learn to perform optimally across all settings, steering the field towards models that are more honest about their uncertainty [1].</p>
</li>
</ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="row margin-top--sm theme-doc-footer-tags-row"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/notes/tags/math">math</a></li><li class="tag_QGVx"><a rel="tag" class="tag_zVej tagRegular_sFm0" href="/notes/tags/llm">llm</a></li></ul></div></div><div class="row margin-top--sm theme-doc-footer-edit-meta-row"><div class="col"><a href="https://github.com/p-s-vishnu/p-s-vishnu.github.io/tree/main/notes/papers/why models hallucinate.md" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_JAkA"></div></div></footer></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/notes/Languages/Python"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Python</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#tldr" class="table-of-contents__link toc-highlight">TLDR</a></li><li><a href="#technical-points-to-know" class="table-of-contents__link toc-highlight">Technical Points to Know</a></li><li><a href="#important-sections" class="table-of-contents__link toc-highlight">Important Sections</a></li></ul></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title"></div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.linkedin.com/in/psvishnu/" target="_blank" rel="noopener noreferrer" aria-label="LinkedIn" title="LinkedIn"><i class="fa-brands fa-linkedin"></i></a></li><li class="footer__item"><a href="https://medium.com/@psvishnu" target="_blank" rel="noopener noreferrer" aria-label="Medium" title="Medium"><i class="fa-brands fa-medium"></i></a></li><li class="footer__item"><a href="mailto:hellovishnups@gmail.com" aria-label="Contact" title="Contact"><i class="fa-solid fa-envelope"></i></a></li><li class="footer__item"><a href="https://github.com/p-s-vishnu" target="_blank" rel="noopener noreferrer" aria-label="GitHub" title="GitHub"><i class="fa-brands fa-github"></i></a></li><li class="footer__item"><a href="https://www.kaggle.com/psvishnu" target="_blank" rel="noopener noreferrer" aria-label="Kaggle" title="Kaggle"><i class="fa-brands fa-kaggle"></i></a></li><li class="footer__item"><a href="/rss.xml" target="_blank" rel="noopener noreferrer" aria-label="RSS Feed" title="RSS Feed"><i class="fa-solid fa-rss"></i></a></li></ul></div></div></div></footer></div>
</body>
</html>